\section{Field Theory}
\subsection{Extension Fields}
\begin{definition}
	If $E$ is a field and $F$ is a subfield of $E$, we say that $E$ is an \defn{extension} of $F$, or $E$ is a \defn{field extension}. Notation: $E:F$.
\end{definition}

If $F \subset E$ and $\alpha, \beta, \gamma, \dots \in E$, we let $F(\alpha, \beta, \gamma, \dots)$ denote the smallest subfield of $E$ that contains $F$ and the elements $\alpha, \beta, \gamma, \dots$. The field $F(\alpha, \beta, \gamma, \dots)$ is the set of all elements in $E$ that can be obtained from elements of $F$ and $\alpha, \beta, \gamma, \dots$ using the field operations $+$, $-$, $\times$ and $/$.

We say that $F(\alpha, \beta, \gamma, \dots)$ is ``obtained by adjunction or by adjoining $\alpha, \beta, \gamma, \dots$ to $F$'', or ``the field generated by $\alpha, \beta, \gamma, \dots$ over $F$''.

If $F \subset E$ we may regard $E$ as a vector space over $F$.

\begin{definition}
	An extension $E:F$ is called \emph{finite}\index{finite!extension} if $E$ is a finite dimensional vector space over $F$. If $E:F$ is finite then the \defn{degree} $(E:F)$\index{$(E:F)$} of the extension is defined by $(E:F) = \text{dim}_F(E)$.
\end{definition}

\begin{theorem}[The Tower Law]\index{Tower Law}
	Let $F, B, E$ be fields with $F \subset B \subset E$\footnote{Like a \emph{tower} of fields!} such that $B:F$ and $E:B$ are finite extensions. Then $E:F$ is also a finite extension and $(E:F) = (B:F)(E:B)$.
	\begin{proof}
		Let $\{A_1, \dots, A_r\}$ be a basis of $E$ as a $B$-space, and let $\{C_1, \dots, C_s\}$ be a basis of $B$ as an $F$-space.
		
		We claim that $\{C_j A_i\ |\ 1 \leq j \leq s, 1 \leq i \leq r\}$ is a basis of $E$ as an $F$-space.
		
		Since $\{A_1, \dots, A_r\}$ forms a basis of $E$ over $B$, every elements $e \in E$ can be written as
		\[
			e = \sum_{i = 1}^r{x_i A_i} \quad \text{with } x_i \in B.
		\]
		Since $\{C_1, \dots, C_s\}$ forms a basis of $B$ over $F$, each $x_i \in B$ can be written as
		\[
			x_i = \sum_{j = 1}^s{a_{ij} C_j} \quad \text{with } a_{ij} \in F.
		\]
		Therefore the elements $C_j A_i$ generate $E$ as a vector space over $F$. To show these elements are linearly independent, suppose we have
		\[
			\sum_{i = 1}^r{\sum_{j = 1}^s{a_{ij} C_j A_i}} = 0,
		\]
		then we have
		\[
			\sum_{i = 1}^r{\underbrace{\left(\sum_{j = 1}^s{a_{ij} C_j}\right)}_{\in B} A_i} = 0.
		\]
		Since $\{A_1, \dots, A_r\}$ is linearly independent over $B$ and
		\[
			\sum_{j = 1}^s{a_{ij} C_j} \in B \quad \text{for } i = 1, \dots, r,
		\]
		it follows that
		\[
			\sum_{j = 1}^s{a_{ij} C_j} = 0 \quad \text{for } i = 1, \dots, r.
		\]
		But $\{C_1, \dots, C_s\}$ is linearly independent over $F$ and the coefficients $a_{ij}$ are in $F$. It must follow that $a_{ij} = 0$ for all $i \in \{1, \dots, r\}$ and all $j \in \{1, \dots, s\}$.
		
		Hence the products $C_j A_i$ are linearly independent over $F$. Thus, they form a basis of $E$ over $F$ and so we have $(E : F) = (B : F)(E : B)$.
	\end{proof}
\end{theorem}

\begin{corollary}
	If $F \subset F_1 \subset F_2 \subset \dots \subset F_n$ is a tower of finite field extensions, then $F_n : F$ is a finite extension and
	\[
		(F_n : F) = (F_1 : F)(F_2 : F_1)\dots(F_n : F_{n - 1}).
	\]
	\begin{proof}
		By induction, applying the Tower Law repeatedly.
	\end{proof}
\end{corollary}

\subsection{Polynomials}
Let $F$ be a field. Let $F[x]$ denote the ring of polynomials in one indeterminant $x$ with coefficients in $F$. Elements in $F[x]$ are of the form
\[
	f = f(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_k x^k,
\]
$a_i \in F$, $k \geq 0$. Addition and multiplication is taken to be the usual operations.

The degree $\deg{f} = k$ if $a_k \neq 0$. If $f \equiv 0$ then $\deg{f} = -\infty$. For any $f, g \in F[x]$ we have $\deg(fg) = \deg{f} + \deg{g}$.

Notice that the constant polynomials, i.e. the polynomials of degree less than 1, are an isomorphic copy of $F$ in $F[x]$.

\subsubsection{Divisibility}
A polynomial $g \in F[x]$ divides $f \in F[x]$ if $f = gh$ for some $h \in F[x]$. We use the usual notation: $g \mid f$.

Recall the \defn{Division Algorithm} for polynomials: Let $f, g \in F[x]$, $g \neq 0$. Then there exists unique polynomials $q, r \in F[x]$ with $\deg{r} < \deg{g}$ such that $f = qg + r$.

Let $E$ be a field containing another field $F$. A \defn{zero} (or \defn{root}) of a polynomial $f \in F[x]$ is an element $\alpha \in E$ such that $f(\alpha) = 0$. (Note that $f$ may not have any zeros in $F$, but it may have zeros in a larger field. For example, $x^2 + 1$ has not roots in $\R$ but does in $\C$.)

\begin{corollary}[Bezout's Theorem]\index{Bezout's Theorem}
	Let $f \in F[x]$. Then $\alpha \in F$ is a zero of $f$ if and only if $(x - \alpha) \mid f$.
\end{corollary}

\begin{corollary}
	If $f \in F[x]$ with $\deg{f} = n$ then $f$ has at most $n$ zeros in $F$.
\end{corollary}

\begin{definition}
	A \defn{highest common factor} of $f, g \in F[x]$ is a polynomial $d \in F[x]$ such that $d \mid f$ and $d \mid g$, and any other common factor of $f$ and $g$ divides $d$.
	
	Note that $d$ always exists and is unique up to a constant factor.
	
	Notation: We let $d = (f, g)$\index{$(f, g)$|see{highest common factor}} denote the \emph{monic} hcf of $f$ and $g$.
\end{definition}

We have the \defn{Euclidean Algorithm} for finding $d = (f, g)$. Moreover, a direct result of applying the Euclidean Algorithm is that there exist polynomials $s, t \in F[x]$ such that $d = sf + tg$.

\begin{definition}
	A non-constant polynomial $f \in F[x]$ is \defn{reducible} if $f = gh$ for some non-constant polynomials $g, h \in F[x]$. It is \defn{irreducible} if it is not reducible.
\end{definition}

Roughly speaking, an irreducible polynomial cannot be factorised into a product of two polynomials of smaller degree.

Note that all polynomials of degree 1 are irreducible.

\begin{theorem}
	Any non-constant polynomial in $F[x]$ is the product of irreducible polynomials, and the irreducible factors are unique up to constant factors.
	
	(We can think of this like the Fundamental Theorem of Arithmetic for integers.)
\end{theorem}

What are the irreducible polynomials in $F[x]$ for a given field $F$?

For $F = \C$, the answer comes from the \emph{Fundamental Theorem of Algebra}: Every non-constant polynomial $f \in \C[x]$ has a zero in $\C$.

\begin{corollary}
	The irreducible polynomials over $\C$ are the polynomials of degree 1.
	\begin{proof}
		By the Fundamental Theorem of Algebra, simply apply Bezout's Theorem repeatedly.
	\end{proof}
\end{corollary}

\begin{corollary}
	The irreducible polynomials over $\R$ are the polynomials of degree 1 and the polynomials of degree 2 that do not have real roots.
\end{corollary}

For $F = \Q$, things are difficult. We have:

\begin{lemma}[Gauss]\index{Gauss's Lemma}
	A polynomial with integer coefficients is irreducible over $\Q$ if and only if it is irreducible over $\Z$.
	\begin{proof}
		Let $f \in \Z[x]$ be irreducible over $\Q$, so $f = gh$ with $g, h \in \Q[x]$. Let $n$ denote the product of the denominators of the coefficients of $g$ and $h$. Then
		\[
			nf = g_1 h_1
		\]
		where $g_1, h_1 \in \Z[x]$. We write $n = p_1 p_2 \dots p_k$ where $p_i$ is prime for $i = 1, \dots, k$. We will show that we can cancel the primes $p_1, \dots, p_k$ without going beyond $\Z[x]$.
		
		Reducing all coefficients mod $p_1$ gives
		\[
			0 = g_2 h_2 \quad \text{in } \Z_{p_1}[x]
		\]
		where $g_2, h_2$ are the canonical images of $g_1$ and $h_1$ in $\Z_{p_1}[x]$ respectively. Since $\Z_{p_1}[x]$ is an integral domain, we must have either $g_2 = 0$ or $h_2 = 0$. That is, all coefficients of either $g_1$ or $h_1$ are divisible by $p_1$. Therefore, we can divide by $p_1$ to get
		\[
			p_2 \dots p_k f = \tilde{g_1}\tilde{h_1}
		\]
		with $\tilde{g_1}, \tilde{h_1} \in \Z[x]$. Continuing in this way, we eventually obtain a factorisation of $f$ in $\Z[x]$.
	\end{proof}
\end{lemma}

\begin{theorem}[Eisenstein's Criterion\footnote{Ferdinand Eisenstein, not the
  famous film director Sergei Eisenstein. Sergei directed \defn{Battleship
  Potemkin}, which we should all watch before we die. Sadly, however, most
  people don't watch classical films, just this Hollywood shit.}]\index{Eisenstein's Criterion}
	Let $f = a_0 + a_1 x + \dots + a_n x^n \in \Z[x]$, $n \geq 1$, $a_n \neq 0$, and let $p$ be a prime such that
	\begin{enumerate}
		\item $p \nmid a_n$;
		\item $p \mid a_i$ for $i = 0, \dots, n - 1$;
		\item $p^2 \nmid a_0$.
	\end{enumerate}
	Then $f$ is irreducible over $\Q$.
	\begin{proof}
		By Gauss's Lemma, it is sufficient to show that $f$ is irreducible over $\Z$.
		
		Suppose $f = gh$ with $g = b_0 + b_1 x + \dots + b_r x^r$ and $h = c_0 + c_1 x + \dots + c_s x^s$ such that $r + s = n$.
		
		Now $a_0 = b_0 c_0$, so by criterion 2, $p \mid b_0$ or $p \mid c_0$. Criterion 3 gives that $p$ cannot divide both $b_0$ and $c_0$, so assume without loss of generality that $p \mid b_0$ but $p \nmid c_0$.
		
		Note that not all coefficients of $g$ are divisible by $p$, since this would imply that all coefficients of $f$ are divisible by $p$, contradicting criterion 1. So let $b_i$ be the first coefficient of $g$ that is not divisible by $p$. Then we have
		\[
			a_i = b_i c_0 + b_{i - 1} c_i + \dots + b_0 c_i,
		\]
		where $i < n$. This implies that $p \mid b_i c_0$ since $p \mid a_i$ and $p$ also divides $b_{i - 1}, b_{i - 2}, \dots, b_0$. But this is impossible, since $p$ does not divide $b_i$ nor $c_0$.
		
		Therefore $f$ is irreducible over $\Q$.
	\end{proof}
\end{theorem}

\begin{corollary}
	The polynomials $x^n - p$, where $p$ is a prime, are irreducible over $\Q$.
\end{corollary}

\subsection{Algebraic Elements}
\begin{definition}
	An element $\alpha \in E$ is called \defn{algebraic} over $F$ if $\alpha$ is a zero of a polynomial $f \in F[x]$.
\end{definition}

\begin{proposition}
	Let $\alpha \in E$ be algebraic over $F$ and let $f \in F[x]$ be a monic polynomial of smallest possible degree with root $\alpha$. Then
	\begin{enumerate}
		\item $f$ is uniquely determined;
		\item $f$ is irreducible;
		\item any polynomial in $F[x]$ with root $\alpha$ is divisible by $f$.
	\end{enumerate}
	\begin{proof}\hfill
		\begin{enumerate}
			\item Let $g \in F[x]$ be another polynomial with root $\alpha$. Then $\alpha$ will be a root of $f - g \in F[x]$. If $\deg{f} = \deg{g}$ and $g$ is monic then $\deg(f - g) < \deg{f}$. Hence $f - g = 0$ and thus $f = g$, i.e. $f$ is unique.
			\item If $f$ was reducible then $f = gh$ for some $g, h \in F[x]$ ($g, h$ non-constant). But $f(\alpha) = g(\alpha)h(\alpha) = 0$ and so $\alpha$ is either a root of $g$ or $h$, a contradiction.
			\item If $g$ is an arbitrary polynomial with root $\alpha$, write $g = qf + r$ (using the division algorithm) with $\deg{r} < \deg{f}$. Evaluating at $\alpha$ gives
			\[
				0 = g(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha).
			\]
			Hence $r(\alpha) = 0$ and therefore $r = 0$. So $f \mid g$.
		\end{enumerate}
	\end{proof}
\end{proposition}

We say that $f$ is the \defn{minimum polynomial} of $\alpha$ over $F$.

Now let $\deg{f} = n \geq 1$ and consider the set $E_0$ of all elements $\theta
\in E$ of the form
\[
	\theta = g(\alpha) = a_0 + a_1 \alpha + \dots + a_{n - 1} \alpha^{n - 1},
\]
$a_0 + \dots + a_{n - 1} \in F$. In other words, $E_0$ is the set of all 
possible values at $\alpha$ of polynomials of degree $< n$ with coefficients in
 $F$.

\begin{proposition}
	$E_0$ is closed under addition, subtraction and multiplication
	\begin{proof}
		This is clear for addition and subtraction.
		
		We prove $E_0$ is closed under multiplication. Let $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$. Write $gh = qf + r$ with $\deg{r} < \deg{f} = n$ (by the Division Algorithm). Then
		\[
			(gh)(\alpha) = g(\alpha)h(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha) \in E_0.
		\]
		Hence $E_0$ is closed under multiplication.
	\end{proof}
\end{proposition}

\begin{proposition}\label{prop:e0-uniqueness}
	Suppose $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$ and $g(\alpha) = h(\alpha)$. Then $g = h$·
	
	In particular, the elements $a_0, \dots, a_{n - 1}$ are uniquely determined by $\theta$.
	\begin{proof}
		Suppose $g \neq h$. Then $\alpha$ would be a zero of a non-zero polynomial $g - h \in F[x]$ with $\deg{(g - h)} < n = \deg{f}$. But $f$ is the monic polynomial of lowest degree for which $\alpha$ is a zero, so we have a contradiction.
	\end{proof}
\end{proposition}

\begin{proposition} \label{prop:e0-field}
	$E_0$ is a field.
	\begin{proof}
		We need to show that the inverse of every non-zero element in $E_0$ is also in $E_0$.
		
		Let $\theta = g(\alpha)$ with $g \in F[x]$, $g \neq 0$, $\deg{g} < n$. Since $f$ is irreducible, we have $(f, g) = 1$. By the Euclidean Algorithm, there exist polynomials $s, t \in F[x]$ such that $1 = sf + tg$. We may assume that $\deg{t} < n$, otherwise we can replace $t$ with $t_1$ where $t = qf + t_1$ with $\deg{t_1} < n$ (so we get a different $s$ as well). Evaluating at $\alpha$ gives
		\[
			1 = s(\alpha)f(\alpha) + t(\alpha)g(\alpha) = t(\alpha)g(\alpha),
		\]
		where $t(\alpha) \in E_0$. But then $t(\alpha) = \theta^{-1}$. Hence $E_0$ is a field.
	\end{proof}
\end{proposition}

\begin{proposition}
	$E_0 = F(\alpha)$, that is, the smallest field containing $F$ and $\alpha$.
	\begin{proof}
		If a subfield of $E$ contains $F$ and $\alpha$, it must contain all $\theta = g(\alpha)$, i.e. all elements of $E_0$. But these form a field (by Proposition \ref{prop:e0-field}). So $F(\alpha) = E_0$.
	\end{proof}
\end{proposition}

Just one more thing\dots\footnote{I don't normally watch American detective series, but I like Columbo because he is cross-eyed and smokes.}
\begin{proposition}
	$(E_0 : F) = (F(\alpha) : F) = n$.
	\begin{proof}
		The elements $1, \alpha, \alpha^2, \dots, \alpha^{n - 1}$ form a basis of $E_0$ over $F$, since any element of $E_0$ can be written as a unique linear combination of these elements by Proposition \ref{prop:e0-uniqueness} (this is the same as showing it is linearly independent).
	\end{proof}
\end{proposition}

\subsubsection{Summary}
TODO.

\subsection[Kronecker's Construction]{Kronecker's Construction\footnote{The beautiful name of Leopold.}}\index{Kronecker's Construction}
Let $F$ be a field and let $f \in F[x]$ be a (monic) irreducible polynomial with $\deg{f} = n > 1$. Consider
\[
	E_1 = \{g \in F[t] \mid \deg{g} < n\} = \{a_0 + a_1 t + \dots + a_{n - 1}t^{n - 1} \mid a_i \in F\}
\]
$E_1$ is an abelian group with respect to addition. Our aim is to define a multiplication on $E_1$. To this end, for each $g \in F[t]$, let $\overline{g}$ denote the remainder of $g$ after dividing by $f$, i.e. $g = qf + \overline{g}$ with $\deg{\overline{g}} < n$. Note that $\overline{g} \in E_1$ for all $g \in F[t]$, and that $\overline{g} = g$ for all $g \in E_1$. We now define a multiplication on $E_1$ by setting
\[
	g \times h = \overline{gh} \quad \text{for all } g, h \in E_1.
\]

\begin{lemma}
	The remainder map $g \mapsto \overline{g}$ has the properties:
	\begin{enumerate}
		\item $\overline{g + h} = \overline{g} + \overline{h}$;
		\item $\overline{gh} = \overline{g}\overline{h}$;
	\end{enumerate}
	\begin{proof}
		Let $g = q_1 f + r_1$, $h = q_2 f + r_2$ with $\deg{r_1} < n$, $\deg{r_2} < n$, so $\overline{g} = r_1$ and $\overline{h} = r_2$.
		\begin{enumerate}
			\item $g + h = (q_1 + q_2)f + r_1 + r_2$ with $\deg{(r_1 + r_2)} < n$. So $\overline{g + h} = \overline{g} + \overline{h}$.
			\item We have
			\begin{align*}
				\overline{gh} &= \overline{(q_1 f + r_1)(q_2 f + r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f + r_1 r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f} + \overline{r_1 r_2} \\
											&= \overline{r_1 r_2} \\
											&= \overline{\overline{g}\overline{h}}.
			\end{align*}
		\end{enumerate}
	\end{proof}
\end{lemma}

\begin{proposition}
	$E_1$ is a field with respect to addition and multiplication.
	\begin{proof}
		Let $g, h, k \in E_1$. Then
		\begin{align*}
			(g \times h) \times k &= \overline{\overline{gh}\,k} = \overline{\overline{gh}\,\overline{k}} = \overline{(gh)k} = \overline{g(hk)} = \overline{\overline{g}\,\overline{hk}} \\
					&= \overline{g\,\overline{hk}} = g \times (h \times k).
		\end{align*}
		So multiplication is associative. Also,
		\[
			g \times h = \overline{gh} = \overline{hg} = h \times g.
		\]
		Hence multiplication is commutative.
		
		The constant polynomial $1 \in E_1$ is the multiplicative identity. Each non-zero element $g \in E$ has an inverse: since $f$ is irreducible and $\deg{g} < n$, we have $(f, g) = 1$. Hence there exist polynomials $s, t \in F[t]$ such that $sf + tg = 1$. But then $\overline{t} \times g = 1$:
		\begin{align*}
			\overline{t} \times g &= \overline{\overline{t} \times g} = \overline{\overline{t} \times \overline{g}} = \overline{tg} = 0 + \overline{tg} = \overline{sf} + \overline{tg} \\
					&= \overline{sf + tg} = \overline{1} = 1.
		\end{align*}
		Hence $g^{-1} = \overline{t}$.
		
		Finally, the distributivity law holds:
		\begin{align*}
			g \times (h + k) = \overline{g(h + k)} \\
					&= \overline{gh + gk} \\
					&= \overline{gh} + \overline{gk} \\
					&= g \times h + g \times k.
		\end{align*}
		Hence $E_1$ is a field.
	\end{proof}
\end{proposition}

\begin{note}
	If $g, h \in E_1$ \emph{and} $gh \in E_1$, then $gh = g \times h$. In particular, for all constant polynomials $a, b$ we have $a \times b = ab$, i.e. we may identify the field $F$ with the subfield of $E_1$ consisting of all constant polynomials.
\end{note}

Finally, consider the polynomial $f = a_0 + a_1 x + \dots + x^n$. Since $F \subset E_1$, we may evaluate $f$ at arbitrary elements of $E_1$. In particular, for $t \in E_1$, we obtain
\[
	f(t) = \underbrace{a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1}}_{\text{(*)}} + t^n.
\]
The term marked (*) is in $E_1$ and is actually written in \emph{canonical form}. But what is the canonical form of $t^n$? In $E_1$ we have
\[
	t^n = t^{n - 1} \times t = \overline{t^{n - 1} \cdot t} = \overline{t^n}.
\]
But $t^n = 1 \cdot f(t) - (a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$, so $\overline{t^n} = -(a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$. But then it follows that $f(t) = 0$. Hence the element $t \in E_1$ is a zero\footnote{WOW!} of the polynomial $f \in F[x]$.

\subsubsection{Summary of Kronecker's Construction}
$E_1$ is a field containing $F$, and $t \in E_1$ is a zero of the irreducible polynomial $f \in F[x]$. Also, $(E_1 : F) = n$.

\begin{note}
	If $f$ is not irreducible, we can construct $E_1$ in exactly the same way. However, it will not be a field -- instead, it will just be a ring.
	
	For those comfortable with factor rings\footnote{Anyone\dots?}: The field $E_1$ can be identified with the factor ring $F[x] / \langle f \rangle$, where $\langle f \rangle$ is the (principal) ideal domain generated by $f$ in $F[x]$.
\end{note}

Recall that we have considered two field extensions $E_0$ and $E_1$ of a field $F$: Given an algebraic element $\alpha \in E$ (where $E$ is some extension field of $F$) with minimum polynomial $f$ of degree $n$, we looked at
\[
	E_0 = \{g(\alpha) \mid g \in F[x],\ \deg{g} < n\} \subseteq F.
\]
Given just $F$ and the same irreducible polynomial $f \in F[x]$ with $\deg{f} = n$, we also looked at
\[
	E_1 = \{g \mid g \in F[x],\ \deg{g} < n\}.
\]
The field operations in $E_1$ were defined using division by $f$, whereas the field operations in $E_0$ are the given operations of the field $E$.

Now compare $E_0$ and $E_1$: They are very similar -- the only difference is that $E_0$ was constructed using a concrete element $\alpha$ in a given extension $E$ of $F$, whereas $E_1$ was constructed using the indeterminant\footnote{English lession: ``indeterminate'' was used in the lecture and the notes. The words are synonymous, but it seems that ``indeterminant'' is used to refer to variables.} $t$. Otherwise addition and multiplication in $E_0$ and $E_1$ are the same. We can switch from $E_0$ to $E_1$ by replacing $\alpha$ with $t$ (and the other way around).

To make this precise, we recall material from \emph{Algebraic Structures}.

\begin{definition}\label{def:mono-iso-auto-morphism}
	A \defn{monomorphism} from a field $E$ into a field $E'$ is an injective map $\sigma : E \to E'$ such that, for all $\alpha, \beta \in E$,
	\[
		\sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta) \quad \text{and} \quad \sigma(\alpha \beta) = \sigma(\alpha)\sigma(\beta).
	\]
	If $\sigma$ is bijective, it is called an \defn{isomorphism}. An isomorphism from $E$ to $E$ is called an \defn{automorphism}.
\end{definition}

\begin{note}
	Monomorphisms of fields are in fact a special case of ring homomorphisms and we could have defined them as non-zero ring homomorphisms from one field to another. In Definition \ref{def:mono-iso-auto-morphism}, \emph{injective} is equivalent to \emph{non-zero}. We showed this fact in Exercise I.3.
\end{note}

\begin{exercises}[Easy]\hfill
	\begin{itemize}
		\item If $\sigma : E \to E'$ is a monomorphism of fields, then $\sigma(0) = 0$ and $\sigma(1) = 1$.
		\item If $\sigma : E \to E'$ is an isomorphism of fields, then the inverse mapping $\sigma^{-1} : E' \to E$ is also an isomorphism.
	\end{itemize}
\end{exercises}

We say that $E_0$ and $E_1$ are isomorphic. The map $\sigma : E_1 \to E_0$ given by
\[
	\sigma(g) = g(\alpha), \quad \text{for all } g \in E_1,
\]
is an isomorphism.

\begin{exercise}[Useful]
	Verify that $\sigma$ is an isomorphism.
\end{exercise}

\begin{theorem}[L. Kronecker]
	Let $F$ be a field. For every non-constant polynomial $f \in F[x]$ there exists an extension of $F$ in which $f$ has a root.
	\begin{proof}
		Use Kronecker's Construction to get an extension of $F$ in which an irreducible factor of $f$ has a root.
	\end{proof}
\end{theorem}

\begin{theorem}\label{thm:8}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $f$ be an irreducible polynomial in $F[x]$ and let $f'$ be the corresponding polynomial in $F'[x]$ (obtained from $f$ by applying $\sigma$ to all the coefficients in $f$). If $E = F(\beta)$ and $E' = F'(\beta')$, where $f(\beta) = 0$ in $E$ and $f'(\beta') = 0$ in $E'$, then $\sigma$ can be extended to an isomorphism between $E$ and $E'$ such that $\sigma(\beta) = \beta'$.
	\begin{proof}
		Both $E$ and $E'$ are isomorphic to $E_1$.
	\end{proof}
\end{theorem}

\subsection{Splitting Fields}
If $F, B, E$ are fields such that $F \subseteq B \subseteq E$, then we call $B$ an \defn{intermediate field} of the extension $E : F$.

\begin{definition}
	Let $p \in F[x]$ with $\deg{p} \geq 1$. An extension $E$ of $F$ in which $p$ can be factored into linear factors is called a \defn{splitting field} for $p$ over $F$ if such a factorisation cannot be carried out in any proper intermediate field.
\end{definition}

\begin{note}
	$E$ is a splitting field of $p$ over $F$ if and only if the roots of $p$ generate $E$. That is,
	\[
		p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
	\]
	$\alpha_i \in E$, $a \in F$, and
	\[
		E = F(\alpha_1, \dots, \alpha_n).
	\]
\end{note}

Terminology: ``$p$ splits'' is equivalent to ``$p$ can be factored into linear factors''.

\begin{theorem}\label{thm:9}
	Let $F$ be a field, $p \in F[x]$ with $\deg{f} = n \geq 1$. Then there exists a splitting field for $f$ over $F$.
	\begin{proof}
		We first show that there exists an extension of $F$ which contains $n$ (not necessarily distinct) roots of $p$. We proceed by induction on $\deg{p}$.
		
		If $\deg{p} = 1$, i.e. $p$ is linear, then $F$ itself contains a root of $p$.
		
		Now let $\deg{p} = n > 1$. By Kronecker's Theorem, there exists an extension $\hat{E}$ of $F$ that contains a root $\alpha_1$ of $p$. Then $p = (x - \alpha_1)g$ for some $g \in \hat{E}[x]$ and $\deg{g} = n - 1$. By the induction hypothesis, there exists an extension $E$ of $\hat{E}$ that contains all $n - 1$ zeros of $g$ and hence all zeros of $p$. In other words, $p$ splits in $E$ and we have
		\[
			p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		where $\alpha_1, \dots, \alpha_n \in E$, $a \in F$. Then $F(\alpha_1, \dots, \alpha_n)$ is a splitting field of $p$ over $F$.
	\end{proof}
\end{theorem}

\begin{remark}
	Since the splitting field of $p$ over $F$ is of the form $F(\alpha_1, \dots, \alpha_n)$, it can be obtained from $F$ by (at most) $n$ successive adjunctions of algebraic elements. Therefore, each of these successive simple algebraic extensions is of finite degree, and hence by the Tower Law, $E$ is of finite degree over $F$.
	
	In fact, $(E : F) \leq n!$ (Exercise).
\end{remark}

Much deeper than Theorem \ref{thm:9} (and very important):

\begin{theorem}\label{thm:10}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $p$ be a polynomial in $F[x]$ and $p'$ be the corresponding polynomial in $F'[x]$ (obtained from $p$ by applying $\sigma$ to all coefficients of $p$). Let $E$ be a splitting field of $p$ over $F$, and let $E'$ be a splitting field of $p'$ over $F'$. Then $\sigma$ can be extended to an isomorphism between $E$ and $E'$.
	\begin{proof}
		We may assume that $p$ is monic. If $f$ is an irreducible factor of $p$, then $E$ contains all the roots of $f$. Indeed, Suppose $p = fg$ with $f, g \in F[x]$ and
		\[
			p = fg = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		with $\alpha_1, \dots, \alpha_n \in E$. If $\alpha$ is a root of $f$ then
		\[
			p(\alpha) = \underbrace{f(\alpha)}_{= 0}g(\alpha) = (\alpha - \alpha_1)(\alpha - \alpha_2)\dots(\alpha - \alpha_n) = 0.
		\]
		Hence at least one of the factors $(\alpha - \alpha_i) = 0$, therefore $\alpha = \alpha_i \in E$.
		
		Now assume that all roots of $p$ are in $F$. Then $E = F$ and $p$ splits in $F$:
		\[
			p = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
		\]
		$\alpha_1, \dots, \alpha_n \in F$. Then
		\[
			p' = (x - \sigma(\alpha_1))(x - \sigma(\alpha_2))\dots(x - \sigma(\alpha_n)),
		\]
		$\sigma(\alpha_1), \dots, \sigma(\alpha_n) \in F'$.
		
		Consequently, $p'$ splits over $F'$, so $E' = F'$ and then $\sigma$ itself is the required extension.
		
		We now proceed by induction. Suppose the result is true whenever the number of roots of $p$ outside $F$ is less than $n \geq 1$. Suppose that $p$ is a polynomial with $n$ roots outside $F$. We factor $p$ into irreducible factors $f_i \in F[x]$, $1 \leq i \leq m$:
		\[
			p = f_1 f_2 \dots f_m.
		\]
		Now, not all of the irreducibles $f_i$ can be linear, since otherwise all roots of $p$ would be in $F$. Hence we may suppose that $\deg{f_1} = r > 1$.
		
		Let $p' = f'_1 f'_2 \dots f'_m$ be the factorisation of $p'$ given by applying $\sigma$ to $p$. Then the $f'_j$ are irreducible over $F'$, since otherwise the inverse isomorphism $\sigma^{-1} : F' \to F$ would give a factorisation of $f_j$.
		
		Let $\alpha$ be a root of $f_1$ (and hence of $p$). By Theorem \ref{thm:8}, the isomorphism $\sigma$ can be extended to an isomorphism $\sigma_1 : F(\alpha) \to F'(\alpha')$, where $\alpha'$ is a root of $f'_1$ in $E'$. Since $F$ is contained in $F(\alpha)$, we may regard $p$ as a polynomial over $F(\alpha)$, and then $E$ is a splitting field for $p$ over $F(\alpha)$. Of course, $p$ has fewer\footnote{Not `less'.} roots outside of $F(\alpha)$ than outside of $F$.
		
		Similarly for $p'$, $E'$ is a splitting field for $p'$ over $F'(\alpha')$. Hence by the induction hypothesis, the isomorphism $\sigma_1$ can be extended to an isomorphism $\sigma_2 : E \to E'$.
		
		Since $\sigma_1$ is an extension of $\sigma$, and $\sigma_2$ is an extension of $\sigma_1$, we conclude that $\sigma_2$ is an extension of $\sigma$, as required.
	\end{proof}
\end{theorem}

\begin{corollary}
	If $p \in F[x]$, then any two splitting fields of $p$ over $F$ are isomorphic.
	\begin{proof}
		Take $F = F'$ and $\sigma$ to be the identity map in Theorem \ref{thm:10}.
	\end{proof}
\end{corollary}

In view of this corollary, it is justified to speak of ``\emph{the} splitting field of $p$ over $F$''. This field will be unique up to isomorphism.

\subsection{Group Characters}
\begin{definition}
	Let $G$ be a group and let $F$ be a field. A \defn{character}\index{group character} of $G$ in $F$ is a function $\sigma : G \to F$ such that
	\[
		\sigma(xy) = \sigma(x) \sigma(y)
	\]
	for all $x, y \in G$, and $\sigma(x) \neq 0$ for all $x \in G$.
	
	In other words, $\sigma$ is a (group) homomorphism from $G$ into the multiplicative group of the field $F$.
\end{definition}

\begin{definition}
	The characters $\sigma_1, \dots, \sigma_n$ are called \emph{dependent}\index{dependent (characters)} if there exists elements $a_1, \dots, a_n \in F$ (not all zero) such that
	\[
		a_1 \sigma_1(x) + a_2 \sigma_2(x) + \dots + a_n \sigma_n(x) = 0
	\]
	for all $x \in G$. The characters are \emph{independent}\index{independent (characters)} if they are not dependent.
\end{definition}

\begin{note}
	The set of all functions $\varphi : G \to F$. is a vector space uner pointwise addition
	\[
		(\varphi_1 + \varphi_2)(x) = \varphi_1(x) + \varphi_2(x) \quad \text{for all } x \in G
	\]
	and scalar multiplication given by
	\[
		(\alpha\varphi)(x) = \alpha\varphi(x) \quad \text{for all } x \in G, \text{ for all } \alpha \in F.
	\]
	The notion of dependence and independence coincide with `ordinary' linear dependence and independence in this vector space.
\end{note}

\begin{theorem}\label{thm:12}
	Let $G$ be a group and let $\sigma_1, \dots, \sigma_n$ be mutually distinct characters of $G$ in a field $F$. Then $\sigma_1, \dots, \sigma_n$ are independent.
	\begin{note}
		$\sigma_1$ and $\sigma_2$ are distinct if $\sigma_1(x) \neq \sigma_2(x)$ for some $x \in G$.
	\end{note}
	\begin{proof}
		We prove the result by induction on $n$.
		
		One character cannot be dependent since $a_1 \sigma_1(x) = 0$ for all $x \in G$ implies that $a_1 = 0$ (since $\sigma_1(x) \neq 0$ by definition).
		
		Now let $n > 1$. Suppose for a contradiction that
		\begin{equation}\label{eq:thm-12-non-trivial-dependence}
			a_1 \sigma_1(x) + a_2 \sigma_2(x) + \dots + a_n \sigma_n(x) = 0,
		\end{equation}
		for all $x \in G$, is a non-trivial dependence. Then all $a_i$ are non-zero, since otherwise we would have a non-trivial dependence between $n - 1$ (mutually distinct) characters, contradicting the induction hypothesis.
		
		Since $\sigma_1 \neq \sigma_n$, there exists an $\alpha \in G$ such that $\sigma_1(\alpha) \neq \sigma_n(\alpha)$. Multiplying the relation (\ref{eq:thm-12-non-trivial-dependence}) by $a_n^{-1}$ gives
		\begin{equation}\label{eq:hammer-and-sickle}
			b_1 \sigma_1(x) + b_2 \sigma_2(x) + \dots + \sigma_n(x) = 0,
		\end{equation}
		for all $x \in G$, where $b_i = a_n^{-1} a_i$, $i = 1, \dots, n - 1$. Replacing $x$ by $\alpha x$ in this relation gives
		\[
			b_1 \sigma_1(\alpha x) + b_2 \sigma_2(\alpha x) + \dots + \sigma_n(\alpha x) = 0,
		\]
		for all $x \in G$. Hence
		\[
			b_1 \sigma_1(\alpha) \sigma_1(x) + b_2 \sigma_2(\alpha) \sigma_2(x) + \dots + \sigma_n(\alpha) \sigma_n(x) = 0,
		\]
		for all $x \in G$. Now we multiply by $\sigma_n(\alpha)^{-1}$ (we can do this since $\sigma_n(\alpha) \neq 0$) to get
		\[
			\sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha) \sigma_1(x) + \dots + \underbrace{\sigma_n(\alpha)^{-1} \sigma_n(\alpha)}_{= 1} \sigma_n(x) = 0,
		\]
		for all $x \in G$. Subtracting this from (\ref{eq:hammer-and-sickle}) gives
		\begin{align}
			&[b_1 - \sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha)] \sigma_1(x) + \dots \nonumber \\
			& \quad \dots + [b_{n - 1} - \sigma_n(\alpha)^{-1} b_{n - 1} \sigma_{n - 1}(\alpha)] \sigma_{n - 1}(x) = 0, \label{eq:3}
		\end{align}
		for all $x \in G$. We now show that the coefficient at $\sigma_1(x)$ in this relation is not zero: if it is zero then we have
		\[
			b_1 - \sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha) = 0
		\]
		and so
		\[
			b_1 (1 - \sigma_n(\alpha)^{-1} \sigma_1(\alpha)) = 0.
		\]
		Since $b_1 \neq 0$, this gives
		\[
			1 - \sigma_n(\alpha)^{-1} \sigma_1(\alpha) = 0,
		\]
		and hence
		\[
			\sigma_1(\alpha) = \sigma_n(\alpha),
		\]
		a contradiction (to the choice of $\alpha$). Therefore the coefficient at $\sigma_1(x)$ is non-zero, so (\ref{eq:3}) a non-trivial dependence between $n - 1$ mutually distinct characters, contradicting the induction hypothesis.
		
		Hence any set of mutually distinct characters of $G$ in $F$ are independent.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:to-thm-12}
	Suppose $E$ and $E'$ are fields, and $\sigma_1, \dots, \sigma_n$ are mutually distinct monomorphisms from $E$ to $E'$. Then $\sigma_1, \dots, \sigma_n$ are independent.
	
	Here, independent means that there is no non-trivial relation of the form
	\[
		a_1 \sigma_1(x) + \dots + a_n \sigma_n(x) = 0
	\]
	for all $x \in E$, $a_1, \dots, a_n \in E'$ not all zero.
	\begin{proof}
		This follows immediately from Theorem \ref{thm:12} since $E \setminus \{0\}$ is a group under multiplication and the restrictions of the $\sigma_i$ to this group are mutually distinct characters from $E \setminus \{0\}$ to $E'$.
	\end{proof}
\end{corollary}

If $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ is a set of maps from $E$ to itself (i.e. for each $i$ we have $\varphi : E \to E$) then we say that $a \in E$ is a \emph{fixed point} for $\Sigma$ if $\varphi_i(a) = a$ for $i = 1, \dots, n$. It will be convenient to generalise this notion to maps between different sets.

\begin{definition}
	If $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ is a set of maps from $E$ to $E'$ we say that $a \in E$ is a \emph{fixed point}\index{fixed!point} for $\Sigma$ if
	\[
		\varphi_1(a) = \varphi_2(a) = \dots = \varphi_n(a).
	\]
\end{definition}

\begin{note}
	We get the ordinary definition of a fixed point if $E = E'$ and one of the maps $\varphi_i$ is the identity map.
\end{note}

\begin{lemma}
	Let $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ be a set of monomorphisms from a field $E$ to a field $E'$. Then the set of fixed points for $\Sigma$ is a subfield of $E$.
	\begin{proof}
		If $a$ and $b$ are fixed points for $\Sigma$ then we have addition:
		\[
			\varphi_i(a + b) = \varphi_i(a) + \varphi_i(b) = \varphi_j(a) + \varphi_j(b) = \varphi_j(a + b);
		\]
		additive inverses:
		\[
			\varphi_i(-a) = -\varphi_i(a) = -\varphi_j(a) = \varphi_j(-a);
		\]
		and multiplication:
		\[
			\varphi_i(ab) = \varphi_i(a) \varphi_i(b) = \varphi_j(a) \varphi_j(b) = \varphi_j(ab).
		\]
		If $a \neq 0$ then we also have multiplicative inverses:
		\[
			\varphi_i(a^{-1}) = (\varphi_i(a))^{-1} = (\varphi_j(a))^{-1} = \varphi_j(a^{-1}).
		\]
		Therefore the set of fixed points is closed under the field operations, and therefore it is a subfield.
	\end{proof}
\end{lemma}

The subfield of fixed points for $\Sigma$ in $E$ is called the \emph{fixed subfield}\index{fixed!subfield} for $\Sigma$. We denote the fixed subfield for $\Sigma$ by $\varPhi(\Sigma)$ \index{$\varPhi(\Sigma)$|see{fixed subfield}}.

\begin{theorem}\label{thm:13}
	Suppose $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of mutually distinct monomorphisms from a field $E$ to a field $E'$, and let $F = \varPhi(\Sigma)$ be the fixed field for $\Sigma$. Then $(E : F) \geq n$.
	\begin{proof}
		Suppose on the contrary that $(E : F) = r < n$. Let $\{\omega_1, \dots, \omega_r\}$ be a basis of $E$ over $F$, and consider the homogeneous system
		\begin{equation}\label{eq:thm-13-hom-sys}
			\sum_{i = 1}^n{\sigma_i(\omega_j)x_i} = 0 \qquad (j = 1, \dots, r)
		\end{equation}
		of linear equations in $E'$. Since the number $r$ of equations is less than the number of variables $n$, there exists a non-trivial solution, say $x_1, \dots, x_n \in E'$.
		
		Since $\{\omega_1, \dots, \omega_r\}$ is a basis of $E$, for any $\alpha \in E$ there exist elements $a_1, \dots, a_r \in F$ such that
		\[
			\alpha = a_1 \omega_1 + a_2 \omega_2 + \dots + a_r \omega_r.
		\]
		Now we multiply the $j$-th equation in (\ref{eq:thm-13-hom-sys}) by $\sigma_1(a_j)$ ($j = 1, \dots, r$). Since $a_j \in F$ and $F$ is the fixed subfield, we have
		\[
			\sigma_1(a_j) = \sigma_2(a_j) = \dots = \sigma_n(a_j) \qquad (j = 1, \dots, r)
		\]
		and since the $\sigma_i$s are monomorphisms, we have
		\[
			\sigma_i(a_j) \sigma_i(\omega_j) = \sigma(a_j \omega_j).
		\]
		Hence, the system (\ref{eq:thm-13-hom-sys}) turns into
		\[
			\begin{matrix}
				\sigma_1(a_1 \omega_1)x_1 & + & \dots & + & \sigma_n(a_1 \omega_1)x_n & = & 0 \\
				\vdots  &  & \vdots &  & \vdots &  & \vdots \\
				\sigma_1(a_r \omega_r)x_1 & + & \dots & + & \sigma_n(a_r \omega_r)x_n & = & 0.
			\end{matrix}
		\]
		Adding all these equations together and using
		\begin{align*}
			\sigma_i(a_1 \omega_1) + \sigma_i(a_2 \omega_2) + \dots + \sigma_i(a_r \omega_r) &= \sigma_i(a_1 \omega_1 + a_2 \omega_2 + \dots + a_r \omega_r) \\
				&= \sigma_i(\alpha),
		\end{align*}
		we get
		\[
			\sigma_1(\alpha)x_1 + \sigma_2(\alpha)x_2 + \dots + \sigma_n(\alpha)x_n = 0
		\]
		for all $\alpha \in E$. This is a non-trivial relation between $n$ mutually distinct monomorphisms, a contradiction to Corollary \ref{cor:to-thm-12}.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:to-thm-13}
	If $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of mutually distinct automorphisms of a field $E$ and $F = \varPhi(\Sigma)$ is the fixed subfield for $\Sigma$, then $(E : F) \geq n$.
	\begin{proof}
		Immediate from Theorem \ref{thm:13} (with $E' = E$).
	\end{proof}
\end{corollary}

\begin{definition}
	If $F$ is a subfield of a field $E$, and $\sigma$ is an automorphism of $E$, we say that \emph{$\sigma$ leaves $F$ fixed}\index{$\sigma$ leaves $F$ fixed} (or \emph{$\sigma$ fixes $F$}) if $\sigma(a) = a$ for all $a \in F$.
\end{definition}

\begin{proposition}
	The set of all automorphisms of $E$ is a group under composition of maps.
	\begin{proof}
		Let's use the definition of a group.
		\begin{enumerate}
			\item Let $\sigma, \tau$ be automorphisms of $E$. Then for all $a, b \in E$ we have
			\begin{align*}
				\sigma\tau(a + b) &= \sigma(\tau(a + b)) = \sigma(\tau(a) + \tau(b)) \\
					&= \sigma(\tau(a)) + \sigma(\tau(b)) = \sigma\tau(a) + \sigma\tau(b),
			\end{align*}
			and similarly
			\[
				\sigma\tau(ab) = \sigma\tau(a)\sigma\tau(b).
			\]
			Since the composition of bijections is still a bijection, it follows that the set of all automorphisms of $E$ is closed under composition.
			\item The \emph{identity map} on $E$ is the identity element of the group of all automorphisms.
			\item We showed in Example sheet IV, Question 2, that the \emph{inverse} map of an automorphism is also an automorphism.
			\item \emph{Associativity} follows by the fact that composition of functions is associative.
		\end{enumerate}
	\end{proof}
\end{proposition}

\begin{notation}
	$\Gamma(E) = \Aut(E) =$ the group of automorphisms on $E$.\index{$\Gamma(E)$}
\end{notation}

\begin{lemma}
	If $E$ is an extension field of $F$, then the set $\Gamma(E : F)$ of automorphisms of $E$ that fix $F$ is a subgroup of $\Gamma(E)$.
	\begin{proof}
		Let's use the subgroup criterion.
		\begin{enumerate}
			\item Let $\sigma, \tau \in \Gamma(E : F)$, let $a \in F$. Then
			\[
				\sigma\tau(a) = \sigma(\tau(a)) = \sigma(a) = a.
			\]
			So $\sigma\tau \in \Gamma(E : F)$.
			\item Clearly, the identity map of $E$ fixes $F$, so $id|_E \in \Gamma(E : F)$.
			\item Let $\sigma \in \Gamma(E : F)$ and $a \in F$. Then
			\[
				\sigma^{-1}(a) = \sigma^{-1}(\sigma(a)) = \sigma^{-1}\sigma(a) = a.
			\]
		\end{enumerate}
		Hence $\Gamma(E : F) \leq \Gamma(E)$.
	\end{proof}
\end{lemma}

\begin{definition}
	\emph{$\Gamma(E : F)$}\index{$\Gamma(E : F)$|see{Galois group}} is called the \defn{Galois group} of the extension $E : F$.
\end{definition}

\begin{remark}
	All automorphisms of $\Gamma(E : F)$ leave the field $F$ fixed. However, this does not mean that $F$ is the fixed field of $\Gamma(E : F)$; the fixed field may in fact be larger.
\end{remark}

\begin{example}
	Let $E = \Q(\alpha)$, where $\alpha = \sqrt[3]{2}$ is the real cube root of $2$. Then
	\[
		E = \{a + b\alpha + c\alpha^2 \mid a, b, c \in \Q\}.
	\]
	Now let $\sigma \in \Gamma(E : \Q)$. Then applying $\sigma$ to $\alpha^3 = 2$ yields
	\[
		[\sigma(\alpha)]^3 = \sigma(\alpha^3) = \sigma(2) = 2.
	\]
	So $\sigma(\alpha)$ is a cube root of 2. Since $E \subset \R$ and $\alpha$ is the only real number such that $\alpha^3 = 2$, we must have that $\sigma(\alpha) = \alpha$. But then
	\begin{align*}
		\sigma(a + b\alpha + c\alpha^2) &= \sigma(a) + \sigma(b)\sigma(\alpha) + \sigma(c)\sigma(\alpha)^2 \\
			&= a + b\alpha + c\alpha^2,
	\end{align*}
	for all $a, b, c \in \Q$. Hence the only $\Q$-automorphism of $E$ is the identity map. Therefore $\Gamma(E : \Q) = \{1\}$. But, of course, the identity fixes all of $E$, so $\varPhi(\Gamma(E : F)) = E$.
\end{example}

A special class of extensions $E : F$ are the ones where $\varPhi(\Gamma(E : F)) = F$. (The Main Theorem of Galois Theory is about such extensions.)

\begin{definition}
	An extension $E : F$ is called \emph{normal}\index{normal extension} if $F$ is the fixed field of $\Gamma(E : F)$ and $(E : F) < +\infty$.
\end{definition}

Recall that if $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of $n$ automorphisms of $E$, and $F$ is the fixed field of $\Sigma$, then $(E : F) \geq n$ (Corollary \ref{cor:to-thm-13}). If the automorphisms in $\Sigma$ form a group, we can say more.

\begin{theorem}\label{thm:14}
	If $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a finite \emph{group} of $n$ automorphisms of a field $E$, and $F$ is the fixed subfield in $E$, then $(E : F) = n$.
	\begin{proof}
		Since $\Sigma$ is a group, it contains the identity automorphism, say $\sigma_1 = id|_E$.
		
		Suppose for a contradiction that $(E : F) > n$. Then there exist $n + 1$ elements in $E$ that are linearly independent over $F$, say $\sigma_1, \dots, \sigma_{n + 1}$. Consider the homogeneous system
		\begin{equation}\label{eq:thm-14-hom-sys}
			\begin{matrix}
				x_1 \sigma_1(\alpha_1) & + & x_2 \sigma_1(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_1(\alpha_{n + 1}) & = & 0 \\
				x_1 \sigma_2(\alpha_1) & + & x_2 \sigma_2(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_2(\alpha_{n + 1}) & = & 0 \\
				\vdots &  & \vdots &  & \vdots &  & \vdots &  & \vdots \\
				x_1 \sigma_n(\alpha_1) & + & x_2 \sigma_n(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_n(\alpha_{n + 1}) & = & 0.
			\end{matrix}
		\end{equation}
		Since there are more indeterminants than equations, the system has a non-trivial solution, say $x_1, \dots, x_{n + 1} \in E$. Observe that this solution cannot lie entirely in $F$, otherwise the first equation in (\ref{eq:thm-14-hom-sys}) would give (using $\sigma_1 = id|_E$)
		\[
			x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_{n + 1} \alpha_{n + 1} = 0.
		\]
		But the $\alpha_i$s are linearly independent over $F$, so this is not possible.
		
		Among the solutions of (\ref{eq:thm-14-hom-sys}), we choose one that has the fewest non-zero elements. Without loss of generality, we may assume that this is
		\[
			a_1, a_2, \dots, a_r, 0, 0, \dots, 0,
		\]
		with the first $r$ of them non-zero. Note that we must have $r > 1$, since otherwise
		\[
			a_1 \sigma_1(\alpha_1) = a_1 \alpha_1 = 0,
		\]
		which gives that $a_1 = 0$ -- but our solution is non-trivial.
		
		We may also assume that $a_r = 1$ by multiplying the solution by $a_r^{-1}$ if necessary. Thus our system now looks like this:
		\begin{equation}\label{eq:orthodox-cross}
			a_1 \sigma_i(\alpha_1) + a_2 \sigma_i(\alpha_2) + \dots + \sigma_i(\alpha_r) = 0 \qquad (i = 1, \dots, n).
		\end{equation}
		Since at least one of the $a_i$s must lie outside of $F$, we may assume that $a_1 \in E \setminus F$. Hence there is an automorphism $\sigma_k \in \Sigma$ such that $\sigma_k(a_1) \neq a_1$. Since $\Sigma$ is a group, we have that $\Sigma = \{\sigma_k\sigma_1, \sigma_k\sigma_2, \dots, \sigma_k\sigma_n\}$. By applying $\sigma_k$ to all the equations in (\ref{eq:orthodox-cross}) we get
		\[
			\sigma_k(a_1) \sigma_k\sigma_j(\alpha_1) + \sigma_k(a_2) \sigma_k\sigma_j(\alpha_2) + \dots + \sigma_k\sigma_j(\alpha_r) = 0 \qquad (j = 1, \dots, n).
		\]
		By setting $\sigma_k\sigma_j = \sigma_i$ this becomes
		\begin{equation}\label{eq:soviet-star}
			\sigma_k(a_1) \sigma_i(\alpha_1) + \sigma_k(a_2) \sigma_i(\alpha_2) + \dots + \sigma_i(\alpha_r) = 0 \qquad (i = 1, \dots, n).
		\end{equation}
		Subtracting (\ref{eq:soviet-star}) from (\ref{eq:orthodox-cross}) gives
		\[
			[a_1 - \sigma_k(a_1)] \sigma_i(\alpha_1) + \dots + [a_{r - 1} - \sigma_k(a_{r - 1})]\sigma_i(\alpha_{r - 1}) = 0 \qquad (i = 1, \dots, n).
		\]
		In particular, note that the leading term $a_1 - \sigma_k(a_1) \neq 0$, so this is a non-trivial solution of the original system (\ref{eq:thm-14-hom-sys}) with fewer than $r$ non-zero elements, a contradiction.
		
		Hence $(E : F) = n$.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:1}
	If $F$ is a fixed field for a finite group $G$ of automorphisms of $E$, then each automorphism of $E$ that fixes $F$ must belong to $G$.
	\begin{proof}
		By Theorem \ref{thm:14} we have $(E : F) = |G| = n$.
		
		Assume that there is an automorphism $\sigma$ that fixes $F$ but $\sigma \notin G$. Then $F$ would be fixed by $n + 1$ automorphisms, namely $\sigma$ and the $n$ automorphisms in $G$. But this contradicts Corollary \ref{cor:to-thm-13}.
	\end{proof}
\end{corollary}

\begin{corollary}\label{cor:2}
	Distinct finite subgroups of $\Gamma(E)$ have distinct fixed fields.
	\begin{proof}
		Follows immediately from Corollary \ref{cor:1}.
	\end{proof}
\end{corollary}

\begin{corollary}\label{cor:3}
	If $E : F$ is a normal extension with Galois group $\Gamma(E : F)$, then
	\[
		|\Gamma(E : F)| + (E : F).
	\]
	\begin{proof}
		If $E : F$ is normal, then $(E : F)$ is finite and $F$ is the fixed field for $\Gamma(E : F)$. Hence $|\Gamma(E : F)| = (E : F)$.
	\end{proof}
\end{corollary}

\begin{definition}
	A \emph{polynomial} $f \in F[x]$ is \emph{separable}\index{separable!polynomial} if the irreducible factors of $f$ have no repeated roots.
	
	An \emph{element} $\alpha$ in an extension $E$ of $F$ is called \emph{separable over $F$}\index{separable!element over $F$} if it is a root of a separable polynomial in $F[x]$.
	
	An \emph{extension} $E : F$ is called \emph{separable}\index{separable!extention} if each element of $E$ is separable over $F$.
\end{definition}

\begin{example}[A non-separable polynomial]
	Let $F = \Z_2$ and let $E = \Z_2(t)$ be the field of rational functions over $F$. Then
	\[
		f = x^2 + t \in E[x],
	\]
	where $t$ is a linear rational function, is not separable over $E$.
\end{example}
