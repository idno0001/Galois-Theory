\section{Field Theory}
\subsection{Extension Fields}
\begin{definition}
	If $E$ is a field and $F$ is a subfield of $E$, we say that $E$ is an \defn{extension} of $F$, or $E$ is a \defn{field extension}. Notation: $E:F$.
\end{definition}

If $F \subset E$ and $\alpha, \beta, \gamma, \dots \in E$, we let $F(\alpha, \beta, \gamma, \dots)$ denote the smallest subfield of $E$ that contains $F$ and the elements $\alpha, \beta, \gamma, \dots$. The field $F(\alpha, \beta, \gamma, \dots)$ is the set of all elements in $E$ that can be obtained from elements of $F$ and $\alpha, \beta, \gamma, \dots$ using the field operations $+$, $-$, $\times$ and $/$.

We say that $F(\alpha, \beta, \gamma, \dots)$ is ``obtained by adjunction or by adjoining $\alpha, \beta, \gamma, \dots$ to $F$'', or ``the field generated by $\alpha, \beta, \gamma, \dots$ over $F$''.

If $F \subset E$ we may regard $E$ as a vector space over $F$.

\begin{definition}
	An extension $E:F$ is called \emph{finite}\index{finite extension} if $E$ is a finite dimensional vector space over $F$. If $E:F$ is finite then the \defn{degree} $(E:F)$\index{$(E:F)$} of the extension is defined by $(E:F) = \text{dim}_F(E)$.
\end{definition}

\begin{theorem}[The Tower Law]\index{Tower Law}
	Let $F, B, E$ be fields with $F \subset B \subset E$\footnote{Like a \emph{tower} of fields!} such that $B:F$ and $E:B$ are finite extensions. Then $E:F$ is also a finite extension and $(E:F) = (B:F)(E:B)$.
	\begin{proof}
		Let $\{A_1, \dots, A_r\}$ be a basis of $E$ as a $B$-space, and let $\{C_1, \dots, C_s\}$ be a basis of $B$ as an $F$-space.
		
		We claim that $\{C_j A_i\ |\ 1 \leq j \leq s, 1 \leq i \leq r\}$ is a basis of $E$ as an $F$-space.
		
		Since $\{A_1, \dots, A_r\}$ forms a basis of $E$ over $B$, every elements $e \in E$ can be written as
		\[
			e = \sum_{i = 1}^r{x_i A_i} \quad \text{with } x_i \in B.
		\]
		Since $\{C_1, \dots, C_s\}$ forms a basis of $B$ over $F$, each $x_i \in B$ can be written as
		\[
			x_i = \sum_{j = 1}^s{a_{ij} C_j} \quad \text{with } a_{ij} \in F.
		\]
		Therefore the elements $C_j A_i$ generate $E$ as a vector space over $F$. To show these elements are linearly independent, suppose we have
		\[
			\sum_{i = 1}^r{\sum_{j = 1}^s{a_{ij} C_j A_i}} = 0,
		\]
		then we have
		\[
			\sum_{i = 1}^r{\underbrace{\left(\sum_{j = 1}^s{a_{ij} C_j}\right)}_{\in B} A_i} = 0.
		\]
		Since $\{A_1, \dots, A_r\}$ is linearly independent over $B$ and
		\[
			\sum_{j = 1}^s{a_{ij} C_j} \in B \quad \text{for } i = 1, \dots r,
		\]
		it follows that
		\[
			\sum_{j = 1}^s{a_{ij} C_j} = 0 \quad \text{for } i = 1, \dots r.
		\]
		But $\{C_1, \dots, C_s\}$ is linearly independent over $F$ and the coefficients $a_{ij}$ are in $F$. It must follow that $a_{ij} = 0$ for all $i \in \{1, \dots, r\}$ and all $j \in \{1, \dots, s\}$.
		
		Hence the products $C_j A_i$ are linearly independent over $F$. Thus, they form a basis of $E$ over $F$ and so we have $(E : F) = (B : F)(E : B)$.
	\end{proof}
\end{theorem}

\begin{corollary}
	If $F \subset F_1 \subset F_2 \subset \dots \subset F_n$ is a tower of finite field extensions, then $F_n : F$ is a finite extension and
	\[
		(F_n : F) = (F_1 : F)(F_2 : F_1)\dots(F_n : F_{n - 1}).
	\]
	\begin{proof}
		By induction, applying the Tower Law repeatedly.
	\end{proof}
\end{corollary}

\subsection{Polynomials}
Let $F$ be a field. Let $F[x]$ denote the ring of polynomials in one indeterminant $x$ with coefficients in $F$. Elements in $F[x]$ are of the form
\[
	f = f(x) = a_0 a_1 x a_2 x^2 + \dots + a_k x^k,
\]
$a_i \in F$, $k \geq 0$. Addition and multiplication is taken to be the usual operations.

The degree $\deg{f} = k$ if $a_k \neq 0$. If $f \equiv 0$ then $\deg{f} = -\infty$. For any $f, g \in F[x]$ we have $\deg(fg) = \deg{f} + \deg{g}$.

Notice that the constant polynomials, i.e. the polynomials of degree less than 1, are an isomorphic copy of $F$ in $F[x]$.

\subsubsection{Divisibility}
A polynomial $g \in F[x]$ divides $f \in F[x]$ if $f = gh$ for some $h \in F[x]$. We use the usual notation: $g \mid f$.

Recall the \defn{Division Algorithm} for polynomials: Let $f, g \in F[x]$, $g \neq 0$. Then there exists unique polynomials $q, r \in F[x]$ with $\deg{r} < \deg{g}$ such that $f = qg + r$.

Let $E$ be a field containing another field $F$. A \defn{zero} (or \defn{root}) of a polynomial $f \in F[x]$ is an element $\alpha \in E$ such that $f(\alpha) = 0$. (Note that $f$ may not have any zeros in $F$, but it may have zeros in a larger field. For example, $x^2 + 1$ has not roots in $\R$ but does in $\C$.)

\begin{corollary}[Bezout's Theorem]\index{Bezout's Theorem}
	Let $f \in F[x]$. Then $\alpha \in F$ is a zero of $f$ if and only if $(x - a) \mid f$.
\end{corollary}

\begin{corollary}
	If $f \in F[x]$ with $\deg{f} = n$ then $f$ has at most $n$ zeros in $F$.
\end{corollary}

\begin{definition}
	A \defn{highest common factor} of $f, g \in F[x]$ is a polynomial $d \in F[x]$ such that $d \mid f$ and $d \mid g$, and any other common factor of $f$ and $g$ divides $d$.
	
	Note that $d$ always exists and is unique up to a constant factor.
	
	Notation: We let $d = (f, g)$\index{$(f, g)$} denote the \emph{monic} hcf of $f$ and $g$.
\end{definition}

We have the \defn{Euclidean Algorithm} for finding $d = (f, g)$. Moreover, a direct result of applying the Euclidean Algorithm is that there exist polynomials $s, t \in F[x]$ such that $d = sf + tg$.

\begin{definition}
	A non-constant polynomial $f \in F[x]$ is \defn{reducible} if $f = gh$ for some non-constant polynomials $g, h \in F[x]$. It is \defn{irreducible} if it is not reducible.
\end{definition}

Roughly speaking, an irreducible polynomial cannot be factorised into a product of two polynomials of smaller degree.

Note that all polynomials of degree 1 are irreducible.

\begin{theorem}
	Any non-constant polynomial in $F[x]$ is the product of irreducible polynomials, and the irreducible factors are unique up to constant factors.
	
	(We can think of this like the Fundamental Theorem of Arithmetic for integers.)
\end{theorem}

What are the irreducible polynomials in $F[x]$ for a given field $F$?

For $F = \C$, the answer comes from the \emph{Fundamental Theorem of Algebra}: Every non-constant polynomial $f \in \C[x]$ has a zero in $\C$.

\begin{corollary}
	The irreducible polynomials over $\C$ are the polynomials of degree 1.
	\begin{proof}
		By the Fundamental Theorem of Algebra, simply apply Bezout's Theorem repeatedly.
	\end{proof}
\end{corollary}

\begin{corollary}
	The irreducible polynomials over $\R$ are the polynomials of degree 1 and the polynomials of degree 2 that do not have real roots.
\end{corollary}

For $F = \Q$, things are difficult. We have:

\begin{lemma}[Gauss]\index{Gauss's Lemma}
	A polynomial with integer coefficients is irreducible over $\Q$ if and only if it is irreducible over $\Z$.
	\begin{proof}
		Let $f \in \Z[x]$ be irreducible over $\Q$, so $f = gh$ with $g, h \in \Q[x]$. Let $n$ denote the product of the denominators of the coefficients of $g$ and $h$. Then
		\[
			nf = g_1 h_1
		\]
		where $g_1, h_1 \in \Z[x]$. We write $n = p_1 p_2 \dots p_k$ where $p_i$ is prime for $i = 1, \dots, k$. We will show that we can cancel the primes $p_1, \dots, p_k$ without going beyond $\Z[x]$.
		
		Reducing all coefficients mod $p_1$ gives
		\[
			0 = g_2 h_2 \quad \text{in } \Z_{p_1}[x]
		\]
		where $g_2, h_2$ are the canonical images of $g_1$ and $h_1$ in $\Z_{p_1}[x]$ respectively. Since $\Z_{p_1}[x]$ is an integral domain, we must have either $g_2 = 0$ or $h_2 = 0$. That is, all coefficients of either $g_1$ or $h_1$ are divisible by $p_1$. Therefore, we can divide by $p_1$ to get
		\[
			p_2 \dots p_k f = \tilde{g_1}\tilde{h_1}
		\]
		with $\tilde{g_1}, \tilde{h_1} \in \Z[x]$. Continuing in this way, we eventually obtain a factorisation of $f$ in $\Z[x]$.
	\end{proof}
\end{lemma}

\begin{theorem}[Eisenstein's Criterion\footnote{Ferdinand Eisenstein, not the
  famous film director Sergei Eisenstein. Sergei directed \defn{Battleship
  Potemkin}, which we should all watch before we die. Sadly, however, most
  people don't watch classical films, just this Hollywood shit.}]\index{Eisenstein's Criterion}
	Let $f = a_0 + a_1 x + \dots + a_n x^n \in \Z[x]$, $n \geq 1$, $a_n \neq 0$, and let $p$ be a prime such that
	\begin{enumerate}
		\item $p \nmid a_n$;
		\item $p \mid a_i$ for $i = 0, \dots, n - 1$;
		\item $p^2 \nmid a_0$.
	\end{enumerate}
	Then $f$ is irreducible over $\Q$.
	\begin{proof}
		By Gauss's Lemma, it is sufficient to show that $f$ is irreducible over $\Z$.
		
		Suppose $f = gh$ with $g = b_0 + b_1 x + \dots + b_r x^r$ and $h = c_0 + c_1 x + \dots + c_s x^s$ such that $r + s = n$.
		
		Now $a_0 = b_0 c_0$, so by criterion 2, $p \mid b_0$ or $p \mid c_0$. Criterion 3 gives that $p$ cannot divide both $b_0$ and $c_0$, so assume without loss of generality that $p \mid b_0$ but $p \nmid c_0$.
		
		Note that not all coefficients of $g$ are divisible by $p$, since this would imply that all coefficients of $f$ are divisible by $p$, contradicting criterion 1. So let $b_i$ be the first coefficient of $g$ that is not divisible by $p$. Then we have
		\[
			a_i = b_i c_0 + b_{i - 1} c_i + \dots + b_0 c_i,
		\]
		where $i < n$. This implies that $p \mid b_i c_0$ since $p \mid a_i$ and $p$ also divides $b_{i - 1}, b_{i - 2}, \dots, b_0$. But this is impossible, since $p$ does not divide $b_i$ nor $c_0$.
		
		Therefore $f$ is irreducible over $\Q$.
	\end{proof}
\end{theorem}

\begin{corollary}
	The polynomials $x^n - p$, where $p$ is a prime, are irreducible over $\Q$.
\end{corollary}

\subsection{Algebraic Elements}
\begin{definition}
	An element $\alpha \in E$ is called \defn{algebraic} over $F$ if $\alpha$ is a zero of a polynomial $f \in F[x]$.
\end{definition}

\begin{proposition}
	Let $\alpha \in E$ be algebraic over $F$ and let $f \in F[x]$ be a monic polynomial of smallest possible degree with root $\alpha$. Then
	\begin{enumerate}
		\item $f$ is uniquely determined;
		\item $f$ is irreducible;
		\item any polynomial in $F[x]$ with root $\alpha$ is divisible by $f$.
	\end{enumerate}
	\begin{proof}\hfill
		\begin{enumerate}
			\item Let $g \in F[x]$ be another polynomial with root $\alpha$. Then $\alpha$ will be a root of $f - g \in F[x]$. If $\deg{f} = \deg{g}$ and $g$ is monic then $\deg(f - g) < \deg{f}$. Hence $f - g = 0$ and thus $f = g$, i.e. $f$ is unique.
			\item If $f$ was reducible then $f = gh$ for some $g, h \in F[x]$ ($g, h$ non-constant). But $f(\alpha) = g(\alpha)h(\alpha) = 0$ and so $\alpha$ is either a root of $g$ or $h$, a contradiction.
			\item If $g$ is an arbitrary polynomial with root $\alpha$, write $g = qf + r$ (using the division algorithm) with $\deg{r} < \deg{f}$. Evaluating at $\alpha$ gives
			\[
				0 = g(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha).
			\]
			Hence $r(\alpha) = 0$ and therefore $r = 0$. So $f \mid g$.
		\end{enumerate}
	\end{proof}
\end{proposition}

We say that $f$ is the \defn{minimum polynomial} of $\alpha$ over $F$.

Now let $\deg{f} = n \geq 1$ and consider the set $E_0$ of all elements $\theta
\in E$ of the form
\[
	\theta = g(\alpha) = a_0 + a_1 \alpha + \dots + a_{n - 1} \alpha^{n - 1},
\]
$a_0 + \dots + a_{n - 1} \in F$. In other words, $E_0$ is the set of all 
possible values at $\alpha$ of polynomials of degree $< n$ with coefficients in
 $F$.

\begin{proposition}
	$E_0$ is closed under addition, subtraction and multiplication
	\begin{proof}
		This is clear for addition and subtraction.
		
		We prove $E_0$ is closed under multiplication. Let $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$. Write $gh = qf + r$ with $\deg{r} < \deg{f} = n$ (by the Division Algorithm). Then
		\[
			(gh)(\alpha) = g(\alpha)h(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha) \in E_0.
		\]
		Hence $E_0$ is closed under multiplication.
	\end{proof}
\end{proposition}

\begin{proposition}\label{prop:e0-uniqueness}
	Suppose $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$ and $g(\alpha) = h(\alpha)$. Then $g = h$Â·
	
	In particular, the elements $a_0, \dots, a_{n - 1}$ are uniquely determined by $\theta$.
	\begin{proof}
		Suppose $g \neq h$. Then $\alpha$ would be a zero of a non-zero polynomial $g - h \in F[x]$ with $\deg{(g - h)} < n = \deg{f}$. But $f$ is the monic polynomial of lowest degree for which $\alpha$ is a zero, so we have a contradiction.
	\end{proof}
\end{proposition}

\begin{proposition} \label{prop:e0-field}
	$E_0$ is a field.
	\begin{proof}
		We need to show that the inverse of every non-zero element in $E_0$ is also in $E_0$.
		
		Let $\theta = g(\alpha)$ with $g \in F[x]$, $g \neq 0$, $\deg{g} < n$. Since $f$ is irreducible, we have $(f, g) = 1$. By the Euclidean Algorithm, there exist polynomials $s, t \in F[x]$ such that $1 = sf + tg$. We may assume that $\deg{t} < n$, otherwise we can replace $t$ with $t_1$ where $t = qf + t_1$ with $\deg{t_1} < n$ (so we get a different $s$ as well). Evaluating at $\alpha$ gives
		\[
			1 = s(\alpha)f(\alpha) + t(\alpha)g(\alpha) = t(\alpha)g(\alpha),
		\]
		where $t(\alpha) \in E_0$. But then $t(\alpha) = \theta^{-1}$. Hence $E_0$ is a field.
	\end{proof}
\end{proposition}

\begin{proposition}
	$E_0 = F(\alpha)$, that is, the smallest field containing $F$ and $\alpha$.
	\begin{proof}
		If a subfield of $E$ contains $F$ and $\alpha$, it must contain all $\theta = g(\alpha)$, i.e. all elements of $E_0$. But these form a field (by Proposition \ref{prop:e0-field}). So $F(\alpha) = E_0$.
	\end{proof}
\end{proposition}

Just one more thing\dots\footnote{I don't normally watch American detective series, but I like Columbo because he is cross-eyed and smokes.}
\begin{proposition}
	$(E_0 : F) = (F(\alpha) : F) = n$.
	\begin{proof}
		The elements $1, \alpha, \alpha^2, \dots, \alpha^{n - 1}$ form a basis of $E_0$ over $F$, since any element of $E_0$ can be written as a unique linear combination of these elements by Proposition \ref{prop:e0-uniqueness} (this is the same as showing it is linearly independent).
	\end{proof}
\end{proposition}

\subsubsection{Summary}
TODO.

\subsection[Kronecker's Construction]{Kronecker's Construction\footnote{The beautiful name of Leopold.}}\index{Kronecker's Construction}
Let $F$ be a field and let $f \in F[x]$ be a (monic) irreducible polynomial with $\deg{f} = n > 1$. Consider
\[
	E_1 = \{g \in F[t] \mid \deg{g} < n\} = \{a_0 + a_1 t + \dots + a_{n - 1}t^{n - 1} \mid a_i \in F\}
\]
$E_1$ is an abelian group with respect to addition. Our aim is to define a multiplication on $E_1$. To this end, for each $g \in F[t]$, let $\overline{g}$ denote the remainder of $g$ after dividing by $f$, i.e. $g = qf + \overline{g}$ with $\deg{\overline{g}} < n$. Note that $\overline{g} \in E_1$ for all $g \in F[t]$, and that $\overline{g} = g$ for all $g \in E_1$. We now define a multiplication on $E_1$ by setting
\[
	g \times h = \overline{gh} \quad \text{for all } g, h \in E_1.
\]

\begin{lemma}
	The remainder map $g \mapsto \overline{g}$ has the properties:
	\begin{enumerate}
		\item $\overline{g + h} = \overline{g} + \overline{h}$;
		\item $\overline{gh} = \overline{g}\overline{h}$;
	\end{enumerate}
	\begin{proof}
		Let $g = q_1 f + r_1$, $h = q_2 f + r_2$ with $\deg{r_1} < n$, $\deg{r_2} < n$, so $\overline{g} = r_1$ and $\overline{h} = r_2$.
		\begin{enumerate}
			\item $g + h = (q_1 + q_2)f + r_1 + r_2$ with $\deg{(r_1 + r_2)} < n$. So $\overline{g + h} = \overline{g} + \overline{h}$.
			\item We have
			\begin{align*}
				\overline{gh} &= \overline{(q_1 f + r_1)(q_2 f + r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f + r_1 r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f} + \overline{r_1 r_2} \\
											&= \overline{r_1 r_2} \\
											&= \overline{\overline{g}\overline{h}}.
			\end{align*}
		\end{enumerate}
	\end{proof}
\end{lemma}

\begin{proposition}
	$E_1$ is a field with respect to addition and multiplication.
	\begin{proof}
		Let $g, h, k \in E_1$. Then
		\begin{align*}
			(g \times h) \times k &= \overline{\overline{gh}\,k} = \overline{\overline{gh}\,\overline{k}} = \overline{(gh)k} = \overline{g(hk)} = \overline{\overline{g}\,\overline{hk}} \\
					&= \overline{g\,\overline{hk}} = g \times (h \times k).
		\end{align*}
		So multiplication is associative. Also,
		\[
			g \times h = \overline{gh} = \overline{hg} = h \times g.
		\]
		Hence multiplication is commutative.
		
		The constant polynomial $1 \in E_1$ is the multiplicative identity. Each non-zero element $g \in E$ has an inverse: since $f$ is irreducible and $\deg{g} < n$, we have $(f, g) = 1$. Hence there exist polynomials $s, t \in F[t]$ such that $sf + tg = 1$. But then $\overline{t} \times g = 1$:
		\begin{align*}
			\overline{t} \times g &= \overline{\overline{t} \times g} = \overline{\overline{t} \times \overline{g}} = \overline{tg} = 0 + \overline{tg} = \overline{sf} + \overline{tg} \\
					&= \overline{sf + tg} = \overline{1} = 1.
		\end{align*}
		Hence $g^{-1} = \overline{t}$.
		
		Finally, the distributitivy law holds:
		\begin{align*}
			g \times (h + k) = \overline{g(h + k)} \\
					&= \overline{gh + gk} \\
					&= \overline{gh} + \overline{gk} \\
					&= g \times h + g \times k.
		\end{align*}
		Hence $E_1$ is a field.
	\end{proof}
\end{proposition}

\begin{note}
	If $g, h \in E_1$ \emph{and} $gh \in E_1$, then $gh = g \times h$. In particular, for all constant polynomials $a, b$ we have $a \times b = ab$, i.e. we may identify the field $F$ with the subfield of $E_1$ consisting of all constant polynomials.
\end{note}

Finally, consider the polynomial $f = a_0 + a_1 x + \dots + x^n$. Since $F \subset E_1$, we may evaluate $f$ at arbitrary elements of $E_1$. In particular, for $t \in E_1$, we obtain
\[
	f(t) = \underbrace{a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1}}_{\text{(*)}} + t^n.
\]
The term marked (*) is in $E_1$ and is actually written in \emph{canonical form}. But what is the canonical form of $t^n$? In $E_1$ we have
\[
	t^n = t^{n - 1} \times t = \overline{t^{n - 1} \cdot t} = \overline{t^n}.
\]
But $t^n = 1 \cdot f(t) - (a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$, so $\overline{t^n} = -(a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$. But then it follows that $f(t) = 0$. Hence the element $t \in E_1$ is a zero\footnote{WOW!} of the polynomial $f \in F[x]$.

\subsubsection{Summary of Kronecker's Construction}
$E_1$ is a field containing $F$, and $t \in E_1$ is a zero of the irreducible polynomial $f \in F[x]$. Also, $(E_1 : F) = n$.

\begin{note}
	If $f$ is not irreducible, we can construct $E_1$ in exactly the same way. However, it will not be a field -- instead, it will just be a ring.
	
	For those comfortable with factor rings\footnote{Anyone\dots?}: The field $E_1$ can be identified with the factor ring $F[x] / \langle f \rangle$, where $\langle f \rangle$ is the (principal) ideal domain generated by $f$ in $F[x]$.
\end{note}

Recall that we have considered two field extensions $E_0$ and $E_1$ of a field $F$: Given an algebraic element $\alpha \in E$ (where $E$ is some extension field of $F$) with minimum polynomial $f$ of degree $n$, we looked at
\[
	E_0 = \{g(\alpha) \mid g \in F[x],\ \deg{g} < n\} \subseteq F.
\]
Given just $F$ and the same irreducible polynomial $f \in F[x]$ with $\deg{f} = n$, we also looked at
\[
	E_1 = \{g \mid g \in F[x],\ \deg{g} < n\}.
\]
The field operations in $E_1$ were defined using division by $f$, whereas the field operations in $E_0$ are the given operations of the field $E$.

Now compare $E_0$ and $E_1$: They are very similar -- the only difference is that $E_0$ was constructed using a concrete element $\alpha$ in a given extension $E$ of $F$, whereas $E_1$ was constructed using the indeterminant\footnote{English lession: ``indeterminate'' was used in the lecture and the notes. The words are synonymous, but it seems that ``indeterminant'' is used to refer to variables.} $t$. Otherwise addition and multiplication in $E_0$ and $E_1$ are the same. We can switch from $E_0$ to $E_1$ by replacing $\alpha$ with $t$ (and the other way around).

To make this precise, we recall material from \emph{Algebraic Structures}.

\begin{definition}\label{def:mono-iso-auto-morphism}
	A \defn{monomorphism} from a field $E$ into a field $E'$ is an injective map $\sigma : E \to E'$ such that, for all $\alpha, \beta \in E$,
	\[
		\sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta) \quad \text{and} \quad \sigma(\alpha \beta) = \sigma(\alpha)\sigma(\beta).
	\]
	If $\sigma$ is bijective, it is called an \defn{isomorphism}. An isomorphism from $E$ to $E$ is called an \defn{automorphism}.
\end{definition}

\begin{note}
	Monomorphisms of fields are in fact a special case of ring homomorphisms and we could have defined them as non-zero ring homomorphisms from one field to another. In Definition \ref{def:mono-iso-auto-morphism}, \emph{injective} is equivalent to \emph{non-zero}. We showed this fact in Exercise I.3.
\end{note}

\begin{exercises}[Easy]\hfill
	\begin{itemize}
		\item If $\sigma : E \to E'$ is a monomorphism of fields, then $\sigma(0) = 0$ and $\sigma(1) = 1$.
		\item If $\sigma : E \to E'$ is an isomorphism of fields, then the inverse mapping $\sigma^{-1} : E' \to E$ is also an isomorphism.
	\end{itemize}
\end{exercises}

We say that $E_0$ and $E_1$ are isomorphic. The map $\sigma : E_1 \to E_0$ given by
\[
	\sigma(g) = g(\alpha), \quad \text{for all } g \in E_1,
\]
is an isomorphism.

\begin{exercise}[Useful]
	Verify that $\sigma$ is an isomorphism.
\end{exercise}

\begin{theorem}[L. Kronecker]
	Let $F$ be a field. For every non-constant polynomial $f \in F[x]$ there exists an extension of $F$ in which $f$ has a root.
	\begin{proof}
		Use Kronecker's Construction to get an extension of $F$ in which an irreducible factor of $f$ has a root.
	\end{proof}
\end{theorem}

\begin{theorem}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $f$ be an irreducible polynomial in $F[x]$ and let $f'$ be the corresponding polynomial in $F'[x]$ (obtained from $f$ by applying $\sigma$ to all the coefficients in $f$). If $E = F(\beta)$ and $E' = F'(\beta')$, where $f(\beta) = 0$ in $E$ and $f'(\beta') = 0$ in $E'$, then $\sigma$ can be extended to an isomorphism between $E$ and $E'$ such that $\sigma(\beta) = \beta'$.
	\begin{proof}
		Both $E$ and $E'$ are isomorphic to $E_1$.
	\end{proof}
\end{theorem}

\subsection{Splitting Fields}
If $F, B, E$ are fields such that $F \subseteq B \subseteq E$, then we call $B$ an \defn{intermediate field} of the extension $E : F$.

\begin{definition}
	Let $p \in F[x]$ with $\deg{p} \geq 1$. An extension $E$ of $F$ in which $p$ can be factored into linear factors is called a \defn{splitting field} for $p$ over $F$ if such a factorisation cannot be carried out in any proper intermediate field.
\end{definition}

\begin{note}
	$E$ is a splitting field of $p$ over $F$ if and only if the roots of $p$ generate $E$. That is,
	\[
		p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
	\]
	$\alpha_i \in E$, $a \in F$, and
	\[
		E = F(\alpha_1, \dots, \alpha_n).
	\]
\end{note}

Terminology: ``$p$ splits'' is equivalent to ``$p$ can be factored into linear factors''.

\begin{theorem}\label{thm:9}
	Let $F$ be a field, $p \in F[x]$ with $\deg{f} = n \geq 1$. Then there exists a splitting field for $f$ over $F$.
	\begin{proof}
		We first show that there exists an extension of $F$ which contains $n$ (not necessarily distinct) roots of $p$. We proceed by induction on $\deg{p}$.
		
		If $\deg{p} = 1$, i.e. $p$ is linear, then $F$ itself contains a root of $p$.
		
		Now let $\deg{p} = n > 1$. By Kronecker's Theorem, there exists an extension $\hat{E}$ of $F$ that contains a root $\alpha_1$ of $p$. Then $p = (x - \alpha_1)g$ for some $g \in \hat{E}[x]$ and $\deg{g} = n - 1$. By the induction hypothesis, there exists an extension $E$ of $\hat{E}$ that contains all $n - 1$ zeros of $g$ and hence all zeros of $p$. In other words, $p$ splits in $E$ and we have
		\[
			p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		where $\alpha_1, \dots, \alpha_n \in E$, $a \in F$. Then $F(\alpha_1, \dots, \alpha_n)$ is a splitting field of $p$ over $F$.
	\end{proof}
\end{theorem}

\begin{remark}
	Since the splitting field of $p$ over $F$ is of the form $F(\alpha_1, \dots, \alpha_n)$, it can be obtained from $F$ by (at most) $n$ successive adjunctions of algebraic elements. Therefore, each of these successive simple algebraic extensions is of finite degree, and hence by the Tower Law, $E$ is of finite degree over $F$.
	
	In fact, $(E : F) \leq n!$ (Exercise).
\end{remark}

Much deeper than Theorem \ref{thm:9} (and very important):

\begin{theorem}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $p$ be a polynomial in $F[x]$ and $p'$ be the corresponding polynomial in $F'[x]$ (obtained from $p$ by applying $\sigma$ to all coefficients of $p$). Let $E$ be a splitting field of $p$ over $F$, and let $E'$ be a splitting field of $p'$ over $F'$. Then $\sigma$ can be extended to an isomorphism between $E$ and $E'$.
	\begin{proof}
		We may assume that $p$ is monic. If $f$ is an irreducible factor of $p$, then $E$ contains all the roots of $f$. Indeed, Suppose $p = fg$ with $f, g \in F[x]$ and
		\[
			p = fg = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		with $\alpha_1, \dots, \alpha_n \in E$. If $\alpha$ is a root of $f$ then
		\[
			p(\alpha) = \underbrace{f(\alpha)}_{= 0}g(\alpha) = (\alpha - \alpha_1)(\alpha - \alpha_2)\dots(\alpha - \alpha_n) = 0.
		\]
		Hence at least one of the factors $(\alpha - \alpha_i) = 0$, therefore $\alpha = \alpha_i \in E$.
		
		Now assume that all roots of $p$ are in $F$. Then $E = F$ and $p$ splits in $F$:
		\[
			p = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
		\]
		$\alpha_1, \dots, \alpha_n \in F$. Then
		\[
			p' = (x - \sigma(\alpha_1))(x - \sigma(\alpha_2))\dots(x - \sigma(\alpha_n)),
		\]
		$\sigma(\alpha_1), \dots, \sigma(\alpha_n) \in F'$.
		
		Consequently, $p'$ splits over $F'$, so $E' = F'$ and then $\sigma$ itself is the required extension.
	\end{proof}
\end{theorem}
