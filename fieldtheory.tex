\section{Field Theory}
\subsection{Extension Fields}
\begin{definition}
	If $E$ is a field and $F$ is a subfield of $E$, we say that $E$ is an \defn{extension} of $F$, or $E$ is a \defn{field extension}. Notation: $E:F$.
\end{definition}

If $F \subset E$ and $\alpha, \beta, \gamma, \dots \in E$, we let $F(\alpha, \beta, \gamma, \dots)$ denote the smallest subfield of $E$ that contains $F$ and the elements $\alpha, \beta, \gamma, \dots$. The field $F(\alpha, \beta, \gamma, \dots)$ is the set of all elements in $E$ that can be obtained from elements of $F$ and $\alpha, \beta, \gamma, \dots$ using the field operations $+$, $-$, $\times$ and $/$.

We say that $F(\alpha, \beta, \gamma, \dots)$ is ``obtained by adjunction or by adjoining $\alpha, \beta, \gamma, \dots$ to $F$'', or ``the field generated by $\alpha, \beta, \gamma, \dots$ over $F$''.

If $F \subset E$ we may regard $E$ as a vector space over $F$.

\begin{definition}
	An extension $E:F$ is called \emph{finite}\index{finite!extension} if $E$ is a finite dimensional vector space over $F$. If $E:F$ is finite then the \defn{degree} $(E:F)$\index{$(E:F)$} of the extension is defined by $(E:F) = \text{dim}_F(E)$.
\end{definition}

\begin{theorem}[The Tower Law]\index{Tower Law}
	Let $F, B, E$ be fields with $F \subset B \subset E$\footnote{Like a \emph{tower} of fields!} such that $B:F$ and $E:B$ are finite extensions. Then $E:F$ is also a finite extension and $(E:F) = (B:F)(E:B)$.
	\begin{proof}
		Let $\{A_1, \dots, A_r\}$ be a basis of $E$ as a $B$-space, and let $\{C_1, \dots, C_s\}$ be a basis of $B$ as an $F$-space.
		
		We claim that $\{C_j A_i\ |\ 1 \leq j \leq s, 1 \leq i \leq r\}$ is a basis of $E$ as an $F$-space.
		
		Since $\{A_1, \dots, A_r\}$ forms a basis of $E$ over $B$, every elements $e \in E$ can be written as
		\[
			e = \sum_{i = 1}^r{x_i A_i} \quad \text{with } x_i \in B.
		\]
		Since $\{C_1, \dots, C_s\}$ forms a basis of $B$ over $F$, each $x_i \in B$ can be written as
		\[
			x_i = \sum_{j = 1}^s{a_{ij} C_j} \quad \text{with } a_{ij} \in F.
		\]
		Therefore the elements $C_j A_i$ generate $E$ as a vector space over $F$. To show these elements are linearly independent, suppose we have
		\[
			\sum_{i = 1}^r{\sum_{j = 1}^s{a_{ij} C_j A_i}} = 0,
		\]
		then we have
		\[
			\sum_{i = 1}^r{\underbrace{\left(\sum_{j = 1}^s{a_{ij} C_j}\right)}_{\in B} A_i} = 0.
		\]
		Since $\{A_1, \dots, A_r\}$ is linearly independent over $B$ and
		\[
			\sum_{j = 1}^s{a_{ij} C_j} \in B \quad \text{for } i = 1, \dots, r,
		\]
		it follows that
		\[
			\sum_{j = 1}^s{a_{ij} C_j} = 0 \quad \text{for } i = 1, \dots, r.
		\]
		But $\{C_1, \dots, C_s\}$ is linearly independent over $F$ and the coefficients $a_{ij}$ are in $F$. It must follow that $a_{ij} = 0$ for all $i \in \{1, \dots, r\}$ and all $j \in \{1, \dots, s\}$.
		
		Hence the products $C_j A_i$ are linearly independent over $F$. Thus, they form a basis of $E$ over $F$ and so we have $(E : F) = (B : F)(E : B)$.
	\end{proof}
\end{theorem}

\begin{corollary}
	If $F \subset F_1 \subset F_2 \subset \dots \subset F_n$ is a tower of finite field extensions, then $F_n : F$ is a finite extension and
	\[
		(F_n : F) = (F_1 : F)(F_2 : F_1)\dots(F_n : F_{n - 1}).
	\]
	\begin{proof}
		By induction, applying the Tower Law repeatedly.
	\end{proof}
\end{corollary}

\subsection{Polynomials}
Let $F$ be a field. Let $F[x]$ denote the ring of polynomials in one indeterminant $x$ with coefficients in $F$. Elements in $F[x]$ are of the form
\[
	f = f(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_k x^k,
\]
$a_i \in F$, $k \geq 0$. Addition and multiplication is taken to be the usual operations.

The degree $\deg{f} = k$ if $a_k \neq 0$. If $f \equiv 0$ then $\deg{f} = -\infty$. For any $f, g \in F[x]$ we have $\deg(fg) = \deg{f} + \deg{g}$.

Notice that the constant polynomials, i.e. the polynomials of degree less than 1, are an isomorphic copy of $F$ in $F[x]$.

\subsubsection{Divisibility}
A polynomial $g \in F[x]$ divides $f \in F[x]$ if $f = gh$ for some $h \in F[x]$. We use the usual notation: $g \mid f$.

Recall the \defn{Division Algorithm} for polynomials: Let $f, g \in F[x]$, $g \neq 0$. Then there exists unique polynomials $q, r \in F[x]$ with $\deg{r} < \deg{g}$ such that $f = qg + r$.

Let $E$ be a field containing another field $F$. A \defn{zero} (or \defn{root}) of a polynomial $f \in F[x]$ is an element $\alpha \in E$ such that $f(\alpha) = 0$. (Note that $f$ may not have any zeros in $F$, but it may have zeros in a larger field. For example, $x^2 + 1$ has not roots in $\R$ but does in $\C$.)

\begin{corollary}[Bezout's Theorem]\index{Bezout's Theorem}
	Let $f \in F[x]$. Then $\alpha \in F$ is a zero of $f$ if and only if $(x - \alpha) \mid f$.
\end{corollary}

\begin{corollary}
	If $f \in F[x]$ with $\deg{f} = n$ then $f$ has at most $n$ zeros in $F$.
\end{corollary}

\begin{definition}
	A \defn{highest common factor} of $f, g \in F[x]$ is a polynomial $d \in F[x]$ such that $d \mid f$ and $d \mid g$, and any other common factor of $f$ and $g$ divides $d$.
	
	Note that $d$ always exists and is unique up to a constant factor.
	
	Notation: We let $d = (f, g)$\index{$(f, g)$|see{highest common factor}} denote the \emph{monic} hcf of $f$ and $g$.\index{hfc|see{highest common factor}}
\end{definition}

We have the \defn{Euclidean Algorithm} for finding $d = (f, g)$. Moreover, a direct result of applying the Euclidean Algorithm is that there exist polynomials $s, t \in F[x]$ such that $d = sf + tg$.

\begin{definition}
	A non-constant polynomial $f \in F[x]$ is \defn{reducible} if $f = gh$ for some non-constant polynomials $g, h \in F[x]$. It is \defn{irreducible} if it is not reducible.
\end{definition}

Roughly speaking, an irreducible polynomial cannot be factorised into a product of two polynomials of smaller degree.

Note that all polynomials of degree 1 are irreducible.

\begin{theorem}
	Any non-constant polynomial in $F[x]$ is the product of irreducible polynomials, and the irreducible factors are unique up to constant factors.
	
	(We can think of this like the Fundamental Theorem of Arithmetic for integers.)
\end{theorem}

What are the irreducible polynomials in $F[x]$ for a given field $F$?

For $F = \C$, the answer comes from the \emph{Fundamental Theorem of Algebra}: Every non-constant polynomial $f \in \C[x]$ has a zero in $\C$.

\begin{corollary}
	The irreducible polynomials over $\C$ are the polynomials of degree 1.
	\begin{proof}
		By the Fundamental Theorem of Algebra, simply apply Bezout's Theorem repeatedly.
	\end{proof}
\end{corollary}

\begin{corollary}
	The irreducible polynomials over $\R$ are the polynomials of degree 1 and the polynomials of degree 2 that do not have real roots.
\end{corollary}

For $F = \Q$, things are difficult. We have:

\begin{lemma}[Gauss]\index{Gauss's Lemma}
	A polynomial with integer coefficients is irreducible over $\Q$ if and only if it is irreducible over $\Z$.
	\begin{proof}
		Let $f \in \Z[x]$ be irreducible over $\Q$, so $f = gh$ with $g, h \in \Q[x]$. Let $n$ denote the product of the denominators of the coefficients of $g$ and $h$. Then
		\[
			nf = g_1 h_1
		\]
		where $g_1, h_1 \in \Z[x]$. We write $n = p_1 p_2 \dots p_k$ where $p_i$ is prime for $i = 1, \dots, k$. We will show that we can cancel the primes $p_1, \dots, p_k$ without going beyond $\Z[x]$.
		
		Reducing all coefficients mod $p_1$ gives
		\[
			0 = g_2 h_2 \quad \text{in } \Z_{p_1}[x]
		\]
		where $g_2, h_2$ are the canonical images of $g_1$ and $h_1$ in $\Z_{p_1}[x]$ respectively. Since $\Z_{p_1}[x]$ is an integral domain, we must have either $g_2 = 0$ or $h_2 = 0$. That is, all coefficients of either $g_1$ or $h_1$ are divisible by $p_1$. Therefore, we can divide by $p_1$ to get
		\[
			p_2 \dots p_k f = \tilde{g_1}\tilde{h_1}
		\]
		with $\tilde{g_1}, \tilde{h_1} \in \Z[x]$. Continuing in this way, we eventually obtain a factorisation of $f$ in $\Z[x]$.
	\end{proof}
\end{lemma}

\begin{theorem}[Eisenstein's Criterion\footnote{Ferdinand Eisenstein, not the
  famous film director Sergei Eisenstein. Sergei directed \defn{Battleship
  Potemkin}, which we should all watch before we die. Sadly, however, most
  people don't watch classical films, just this Hollywood shit.}]\index{Eisenstein's Criterion}
	Let $f = a_0 + a_1 x + \dots + a_n x^n \in \Z[x]$, $n \geq 1$, $a_n \neq 0$, and let $p$ be a prime such that
	\begin{enumerate}
		\item $p \nmid a_n$;
		\item $p \mid a_i$ for $i = 0, \dots, n - 1$;
		\item $p^2 \nmid a_0$.
	\end{enumerate}
	Then $f$ is irreducible over $\Q$.
	\begin{proof}
		By Gauss's Lemma, it is sufficient to show that $f$ is irreducible over $\Z$.
		
		Suppose $f = gh$ with $g = b_0 + b_1 x + \dots + b_r x^r$ and $h = c_0 + c_1 x + \dots + c_s x^s$ such that $r + s = n$.
		
		Now $a_0 = b_0 c_0$, so by criterion 2, $p \mid b_0$ or $p \mid c_0$. Criterion 3 gives that $p$ cannot divide both $b_0$ and $c_0$, so assume without loss of generality that $p \mid b_0$ but $p \nmid c_0$.
		
		Note that not all coefficients of $g$ are divisible by $p$, since this would imply that all coefficients of $f$ are divisible by $p$, contradicting criterion 1. So let $b_i$ be the first coefficient of $g$ that is not divisible by $p$. Then we have
		\[
			a_i = b_i c_0 + b_{i - 1} c_i + \dots + b_0 c_i,
		\]
		where $i < n$. This implies that $p \mid b_i c_0$ since $p \mid a_i$ and $p$ also divides $b_{i - 1}, b_{i - 2}, \dots, b_0$. But this is impossible, since $p$ does not divide $b_i$ nor $c_0$.
		
		Therefore $f$ is irreducible over $\Q$.
	\end{proof}
\end{theorem}

\begin{corollary}
	The polynomials $x^n - p$, where $p$ is a prime, are irreducible over $\Q$.
\end{corollary}

\subsection{Algebraic Elements}
\begin{definition}
	An element $\alpha \in E$ is called \defn{algebraic} over $F$ if $\alpha$ is a zero of a polynomial $f \in F[x]$.
\end{definition}

\begin{proposition}
	Let $\alpha \in E$ be algebraic over $F$ and let $f \in F[x]$ be a monic polynomial of smallest possible degree with root $\alpha$. Then
	\begin{enumerate}
		\item $f$ is uniquely determined;
		\item $f$ is irreducible;
		\item any polynomial in $F[x]$ with root $\alpha$ is divisible by $f$.
	\end{enumerate}
	\begin{proof}\hfill
		\begin{enumerate}
			\item Let $g \in F[x]$ be another polynomial with root $\alpha$. Then $\alpha$ will be a root of $f - g \in F[x]$. If $\deg{f} = \deg{g}$ and $g$ is monic then $\deg(f - g) < \deg{f}$. Hence $f - g = 0$ and thus $f = g$, i.e. $f$ is unique.
			\item If $f$ was reducible then $f = gh$ for some $g, h \in F[x]$ ($g, h$ non-constant). But $f(\alpha) = g(\alpha)h(\alpha) = 0$ and so $\alpha$ is either a root of $g$ or $h$, a contradiction.
			\item If $g$ is an arbitrary polynomial with root $\alpha$, write $g = qf + r$ (using the division algorithm) with $\deg{r} < \deg{f}$. Evaluating at $\alpha$ gives
			\[
				0 = g(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha).
			\]
			Hence $r(\alpha) = 0$ and therefore $r = 0$. So $f \mid g$.
		\end{enumerate}
	\end{proof}
\end{proposition}

We say that $f$ is the \defn{minimum polynomial} of $\alpha$ over $F$.

Now let $\deg{f} = n \geq 1$ and consider the set $E_0$ of all elements $\theta
\in E$ of the form
\[
	\theta = g(\alpha) = a_0 + a_1 \alpha + \dots + a_{n - 1} \alpha^{n - 1},
\]
$a_0 + \dots + a_{n - 1} \in F$. In other words, $E_0$ is the set of all 
possible values at $\alpha$ of polynomials of degree $< n$ with coefficients in
 $F$.

\begin{proposition}
	$E_0$ is closed under addition, subtraction and multiplication
	\begin{proof}
		This is clear for addition and subtraction.
		
		We prove $E_0$ is closed under multiplication. Let $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$. Write $gh = qf + r$ with $\deg{r} < \deg{f} = n$ (by the Division Algorithm). Then
		\[
			(gh)(\alpha) = g(\alpha)h(\alpha) = q(\alpha)f(\alpha) + r(\alpha) = r(\alpha) \in E_0.
		\]
		Hence $E_0$ is closed under multiplication.
	\end{proof}
\end{proposition}

\begin{proposition}\label{prop:e0-uniqueness}
	Suppose $g, h \in F[x]$ with $\deg{g} < n$, $\deg{h} < n$ and $g(\alpha) = h(\alpha)$. Then $g = h$Â·
	
	In particular, the elements $a_0, \dots, a_{n - 1}$ are uniquely determined by $\theta$.
	\begin{proof}
		Suppose $g \neq h$. Then $\alpha$ would be a zero of a non-zero polynomial $g - h \in F[x]$ with $\deg{(g - h)} < n = \deg{f}$. But $f$ is the monic polynomial of lowest degree for which $\alpha$ is a zero, so we have a contradiction.
	\end{proof}
\end{proposition}

\begin{proposition} \label{prop:e0-field}
	$E_0$ is a field.
	\begin{proof}
		We need to show that the inverse of every non-zero element in $E_0$ is also in $E_0$.
		
		Let $\theta = g(\alpha)$ with $g \in F[x]$, $g \neq 0$, $\deg{g} < n$. Since $f$ is irreducible, we have $(f, g) = 1$. By the Euclidean Algorithm, there exist polynomials $s, t \in F[x]$ such that $1 = sf + tg$. We may assume that $\deg{t} < n$, otherwise we can replace $t$ with $t_1$ where $t = qf + t_1$ with $\deg{t_1} < n$ (so we get a different $s$ as well). Evaluating at $\alpha$ gives
		\[
			1 = s(\alpha)f(\alpha) + t(\alpha)g(\alpha) = t(\alpha)g(\alpha),
		\]
		where $t(\alpha) \in E_0$. But then $t(\alpha) = \theta^{-1}$. Hence $E_0$ is a field.
	\end{proof}
\end{proposition}

\begin{proposition}
	$E_0 = F(\alpha)$, that is, the smallest field containing $F$ and $\alpha$.
	\begin{proof}
		If a subfield of $E$ contains $F$ and $\alpha$, it must contain all $\theta = g(\alpha)$, i.e. all elements of $E_0$. But these form a field (by Proposition \ref{prop:e0-field}). So $F(\alpha) = E_0$.
	\end{proof}
\end{proposition}

Just one more thing\dots\footnote{I don't normally watch American detective series, but I like Columbo because he is cross-eyed and smokes.}
\begin{proposition}
	$(E_0 : F) = (F(\alpha) : F) = n$.
	\begin{proof}
		The elements $1, \alpha, \alpha^2, \dots, \alpha^{n - 1}$ form a basis of $E_0$ over $F$, since any element of $E_0$ can be written as a unique linear combination of these elements by Proposition \ref{prop:e0-uniqueness} (this is the same as showing it is linearly independent).
	\end{proof}
\end{proposition}

\subsection[Kronecker's Construction]{Kronecker's Construction\footnote{The beautiful name of Leopold.}}\index{Kronecker's Construction}
Let $F$ be a field and let $f \in F[x]$ be a (monic) irreducible polynomial with $\deg{f} = n > 1$. Consider
\[
	E_1 = \{g \in F[t] \mid \deg{g} < n\} = \{a_0 + a_1 t + \dots + a_{n - 1}t^{n - 1} \mid a_i \in F\}
\]
$E_1$ is an abelian group with respect to addition. Our aim is to define a multiplication on $E_1$. To this end, for each $g \in F[t]$, let $\overline{g}$ denote the remainder of $g$ after dividing by $f$, i.e. $g = qf + \overline{g}$ with $\deg{\overline{g}} < n$. Note that $\overline{g} \in E_1$ for all $g \in F[t]$, and that $\overline{g} = g$ for all $g \in E_1$. We now define a multiplication on $E_1$ by setting
\[
	g \times h = \overline{gh} \quad \text{for all } g, h \in E_1.
\]

\begin{lemma}
	The remainder map $g \mapsto \overline{g}$ has the properties:
	\begin{enumerate}
		\item $\overline{g + h} = \overline{g} + \overline{h}$;
		\item $\overline{gh} = \overline{g}\overline{h}$;
	\end{enumerate}
	\begin{proof}
		Let $g = q_1 f + r_1$, $h = q_2 f + r_2$ with $\deg{r_1} < n$, $\deg{r_2} < n$, so $\overline{g} = r_1$ and $\overline{h} = r_2$.
		\begin{enumerate}
			\item $g + h = (q_1 + q_2)f + r_1 + r_2$ with $\deg{(r_1 + r_2)} < n$. So $\overline{g + h} = \overline{g} + \overline{h}$.
			\item We have
			\begin{align*}
				\overline{gh} &= \overline{(q_1 f + r_1)(q_2 f + r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f + r_1 r_2)} \\
											&= \overline{(q_1 q_2 f + q_1 r_2 + r_1 q_2)f} + \overline{r_1 r_2} \\
											&= \overline{r_1 r_2} \\
											&= \overline{\overline{g}\overline{h}}.
			\end{align*}
		\end{enumerate}
	\end{proof}
\end{lemma}

\begin{proposition}
	$E_1$ is a field with respect to addition and multiplication.
	\begin{proof}
		Let $g, h, k \in E_1$. Then
		\begin{align*}
			(g \times h) \times k &= \overline{\overline{gh}\,k} = \overline{\overline{gh}\,\overline{k}} = \overline{(gh)k} = \overline{g(hk)} = \overline{\overline{g}\,\overline{hk}} \\
					&= \overline{g\,\overline{hk}} = g \times (h \times k).
		\end{align*}
		So multiplication is associative. Also,
		\[
			g \times h = \overline{gh} = \overline{hg} = h \times g.
		\]
		Hence multiplication is commutative.
		
		The constant polynomial $1 \in E_1$ is the multiplicative identity. Each non-zero element $g \in E$ has an inverse: since $f$ is irreducible and $\deg{g} < n$, we have $(f, g) = 1$. Hence there exist polynomials $s, t \in F[t]$ such that $sf + tg = 1$. But then $\overline{t} \times g = 1$:
		\begin{align*}
			\overline{t} \times g &= \overline{\overline{t} \times g} = \overline{\overline{t} \times \overline{g}} = \overline{tg} = 0 + \overline{tg} = \overline{sf} + \overline{tg} \\
					&= \overline{sf + tg} = \overline{1} = 1.
		\end{align*}
		Hence $g^{-1} = \overline{t}$.
		
		Finally, the distributivity law holds:
		\begin{align*}
			g \times (h + k) = \overline{g(h + k)} \\
					&= \overline{gh + gk} \\
					&= \overline{gh} + \overline{gk} \\
					&= g \times h + g \times k.
		\end{align*}
		Hence $E_1$ is a field.
	\end{proof}
\end{proposition}

\begin{note}
	If $g, h \in E_1$ \emph{and} $gh \in E_1$, then $gh = g \times h$. In particular, for all constant polynomials $a, b$ we have $a \times b = ab$, i.e. we may identify the field $F$ with the subfield of $E_1$ consisting of all constant polynomials.
\end{note}

Finally, consider the polynomial $f = a_0 + a_1 x + \dots + x^n$. Since $F \subset E_1$, we may evaluate $f$ at arbitrary elements of $E_1$. In particular, for $t \in E_1$, we obtain
\[
	f(t) = \underbrace{a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1}}_{\text{(*)}} + t^n.
\]
The term marked (*) is in $E_1$ and is actually written in \emph{canonical form}. But what is the canonical form of $t^n$? In $E_1$ we have
\[
	t^n = t^{n - 1} \times t = \overline{t^{n - 1} \cdot t} = \overline{t^n}.
\]
But $t^n = 1 \cdot f(t) - (a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$, so $\overline{t^n} = -(a_0 + a_1 t + \dots + a_{n - 1} t^{n - 1})$. But then it follows that $f(t) = 0$. Hence the element $t \in E_1$ is a zero\footnote{WOW!} of the polynomial $f \in F[x]$.

\subsubsection{Summary of Kronecker's Construction}
$E_1$ is a field containing $F$, and $t \in E_1$ is a zero of the irreducible polynomial $f \in F[x]$. Also, $(E_1 : F) = n$.

\begin{note}
	If $f$ is not irreducible, we can construct $E_1$ in exactly the same way. However, it will not be a field -- instead, it will just be a ring.
	
	For those comfortable with factor rings\footnote{Anyone\dots?}: The field $E_1$ can be identified with the factor ring $F[x] / \langle f \rangle$, where $\langle f \rangle$ is the (principal) ideal domain generated by $f$ in $F[x]$.
\end{note}

Recall that we have considered two field extensions $E_0$ and $E_1$ of a field $F$: Given an algebraic element $\alpha \in E$ (where $E$ is some extension field of $F$) with minimum polynomial $f$ of degree $n$, we looked at
\[
	E_0 = \{g(\alpha) \mid g \in F[x],\ \deg{g} < n\} \subseteq F.
\]
Given just $F$ and the same irreducible polynomial $f \in F[x]$ with $\deg{f} = n$, we also looked at
\[
	E_1 = \{g \mid g \in F[x],\ \deg{g} < n\}.
\]
The field operations in $E_1$ were defined using division by $f$, whereas the field operations in $E_0$ are the given operations of the field $E$.

Now compare $E_0$ and $E_1$: They are very similar -- the only difference is that $E_0$ was constructed using a concrete element $\alpha$ in a given extension $E$ of $F$, whereas $E_1$ was constructed using the indeterminant\footnote{English lession: ``indeterminate'' was used in the lecture and the notes. The words are synonymous, but it seems that ``indeterminant'' is used to refer to variables.} $t$. Otherwise addition and multiplication in $E_0$ and $E_1$ are the same. We can switch from $E_0$ to $E_1$ by replacing $\alpha$ with $t$ (and the other way around).

To make this precise, we recall material from \emph{Algebraic Structures}.

\begin{definition}\label{def:mono-iso-auto-morphism}
	A \defn{monomorphism} from a field $E$ into a field $E'$ is an injective map $\sigma : E \to E'$ such that, for all $\alpha, \beta \in E$,
	\[
		\sigma(\alpha + \beta) = \sigma(\alpha) + \sigma(\beta) \quad \text{and} \quad \sigma(\alpha \beta) = \sigma(\alpha)\sigma(\beta).
	\]
	If $\sigma$ is bijective, it is called an \defn{isomorphism}. An isomorphism from $E$ to $E$ is called an \defn{automorphism}.
\end{definition}

\begin{note}
	Monomorphisms of fields are in fact a special case of ring homomorphisms and we could have defined them as non-zero ring homomorphisms from one field to another. In Definition \ref{def:mono-iso-auto-morphism}, \emph{injective} is equivalent to \emph{non-zero}. We showed this fact in Exercise I.3.
\end{note}

\begin{exercises}[Easy]\hfill
	\begin{itemize}
		\item If $\sigma : E \to E'$ is a monomorphism of fields, then $\sigma(0) = 0$ and $\sigma(1) = 1$.
		\item If $\sigma : E \to E'$ is an isomorphism of fields, then the inverse mapping $\sigma^{-1} : E' \to E$ is also an isomorphism.
	\end{itemize}
\end{exercises}

We say that $E_0$ and $E_1$ are isomorphic. The map $\sigma : E_1 \to E_0$ given by
\[
	\sigma(g) = g(\alpha), \quad \text{for all } g \in E_1,
\]
is an isomorphism.

\begin{exercise}[Useful]
	Verify that $\sigma$ is an isomorphism.
\end{exercise}

\begin{theorem}[L. Kronecker]
	Let $F$ be a field. For every non-constant polynomial $f \in F[x]$ there exists an extension of $F$ in which $f$ has a root.
	\begin{proof}
		Use Kronecker's Construction to get an extension of $F$ in which an irreducible factor of $f$ has a root.
	\end{proof}
\end{theorem}

\begin{theorem}\label{thm:8}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $f$ be an irreducible polynomial in $F[x]$ and let $f'$ be the corresponding polynomial in $F'[x]$ (obtained from $f$ by applying $\sigma$ to all the coefficients in $f$). If $E = F(\beta)$ and $E' = F'(\beta')$, where $f(\beta) = 0$ in $E$ and $f'(\beta') = 0$ in $E'$, then $\sigma$ can be extended to an isomorphism between $E$ and $E'$ such that $\sigma(\beta) = \beta'$.
	
	\begin{center}
		\def\svgwidth{0.5\columnwidth}
		\inkscape{thm8}
	\end{center}
		
	\begin{proof}
		Both $E$ and $E'$ are isomorphic to $E_1$.
	\end{proof}
\end{theorem}

\subsection{Splitting Fields}
If $F, B, E$ are fields such that $F \subseteq B \subseteq E$, then we call $B$ an \defn{intermediate field} of the extension $E : F$.

\begin{definition}
	Let $p \in F[x]$ with $\deg{p} \geq 1$. An extension $E$ of $F$ in which $p$ can be factored into linear factors is called a \defn{splitting field} for $p$ over $F$ if such a factorisation cannot be carried out in any proper intermediate field.
\end{definition}

\begin{note}
	$E$ is a splitting field of $p$ over $F$ if and only if the roots of $p$ generate $E$. That is,
	\[
		p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
	\]
	$\alpha_i \in E$, $a \in F$, and
	\[
		E = F(\alpha_1, \dots, \alpha_n).
	\]
\end{note}

Terminology: ``$p$ splits'' is equivalent to ``$p$ can be factored into linear factors''.

\begin{theorem}\label{thm:9}
	Let $F$ be a field, $p \in F[x]$ with $\deg{f} = n \geq 1$. Then there exists a splitting field for $f$ over $F$.
	\begin{proof}
		We first show that there exists an extension of $F$ which contains $n$ (not necessarily distinct) roots of $p$. We proceed by induction on $\deg{p}$.
		
		If $\deg{p} = 1$, i.e. $p$ is linear, then $F$ itself contains a root of $p$.
		
		Now let $\deg{p} = n > 1$. By Kronecker's Theorem, there exists an extension $\hat{E}$ of $F$ that contains a root $\alpha_1$ of $p$. Then $p = (x - \alpha_1)g$ for some $g \in \hat{E}[x]$ and $\deg{g} = n - 1$. By the induction hypothesis, there exists an extension $E$ of $\hat{E}$ that contains all $n - 1$ zeros of $g$ and hence all zeros of $p$. In other words, $p$ splits in $E$ and we have
		\[
			p = a(x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		where $\alpha_1, \dots, \alpha_n \in E$, $a \in F$. Then $F(\alpha_1, \dots, \alpha_n)$ is a splitting field of $p$ over $F$.
	\end{proof}
\end{theorem}

\begin{remark}
	Since the splitting field of $p$ over $F$ is of the form $F(\alpha_1, \dots, \alpha_n)$, it can be obtained from $F$ by (at most) $n$ successive adjunctions of algebraic elements. Therefore, each of these successive simple algebraic extensions is of finite degree, and hence by the Tower Law, $E$ is of finite degree over $F$.
	
	In fact, $(E : F) \leq n!$ (Exercise).
\end{remark}

Much deeper than Theorem \ref{thm:9} (and very important):

\begin{theorem}\label{thm:10}
	Let $\sigma : F \to F'$ be an isomorphism of fields, let $p$ be a polynomial in $F[x]$ and $p'$ be the corresponding polynomial in $F'[x]$ (obtained from $p$ by applying $\sigma$ to all coefficients of $p$). Let $E$ be a splitting field of $p$ over $F$, and let $E'$ be a splitting field of $p'$ over $F'$. Then $\sigma$ can be extended to an isomorphism between $E$ and $E'$.
	\begin{proof}
		We may assume that $p$ is monic. If $f$ is an irreducible factor of $p$, then $E$ contains all the roots of $f$. Indeed, Suppose $p = fg$ with $f, g \in F[x]$ and
		\[
			p = fg = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n)
		\]
		with $\alpha_1, \dots, \alpha_n \in E$. If $\alpha$ is a root of $f$ then
		\[
			p(\alpha) = \underbrace{f(\alpha)}_{= 0}g(\alpha) = (\alpha - \alpha_1)(\alpha - \alpha_2)\dots(\alpha - \alpha_n) = 0.
		\]
		Hence at least one of the factors $(\alpha - \alpha_i) = 0$, therefore $\alpha = \alpha_i \in E$.
		
		Now assume that all roots of $p$ are in $F$. Then $E = F$ and $p$ splits in $F$:
		\[
			p = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_n),
		\]
		$\alpha_1, \dots, \alpha_n \in F$. Then
		\[
			p' = (x - \sigma(\alpha_1))(x - \sigma(\alpha_2))\dots(x - \sigma(\alpha_n)),
		\]
		$\sigma(\alpha_1), \dots, \sigma(\alpha_n) \in F'$.
		
		Consequently, $p'$ splits over $F'$, so $E' = F'$ and then $\sigma$ itself is the required extension.
		
		We now proceed by induction. Suppose the result is true whenever the number of roots of $p$ outside $F$ is less than $n \geq 1$. Suppose that $p$ is a polynomial with $n$ roots outside $F$. We factor $p$ into irreducible factors $f_i \in F[x]$, $1 \leq i \leq m$:
		\[
			p = f_1 f_2 \dots f_m.
		\]
		Now, not all of the irreducibles $f_i$ can be linear, since otherwise all roots of $p$ would be in $F$. Hence we may suppose that $\deg{f_1} = r > 1$.
		
		Let $p' = f'_1 f'_2 \dots f'_m$ be the factorisation of $p'$ given by applying $\sigma$ to $p$. Then the $f'_j$ are irreducible over $F'$, since otherwise the inverse isomorphism $\sigma^{-1} : F' \to F$ would give a factorisation of $f_j$.
		
		Let $\alpha$ be a root of $f_1$ (and hence of $p$). By Theorem \ref{thm:8}, the isomorphism $\sigma$ can be extended to an isomorphism $\sigma_1 : F(\alpha) \to F'(\alpha')$, where $\alpha'$ is a root of $f'_1$ in $E'$. Since $F$ is contained in $F(\alpha)$, we may regard $p$ as a polynomial over $F(\alpha)$, and then $E$ is a splitting field for $p$ over $F(\alpha)$. Of course, $p$ has fewer\footnote{Not `less'.} roots outside of $F(\alpha)$ than outside of $F$.
		
		Similarly for $p'$, $E'$ is a splitting field for $p'$ over $F'(\alpha')$. Hence by the induction hypothesis, the isomorphism $\sigma_1$ can be extended to an isomorphism $\sigma_2 : E \to E'$.
		
		Since $\sigma_1$ is an extension of $\sigma$, and $\sigma_2$ is an extension of $\sigma_1$, we conclude that $\sigma_2$ is an extension of $\sigma$, as required.
	\end{proof}
\end{theorem}

\begin{corollary}
	If $p \in F[x]$, then any two splitting fields of $p$ over $F$ are isomorphic.
	\begin{proof}
		Take $F = F'$ and $\sigma$ to be the identity map in Theorem \ref{thm:10}.
	\end{proof}
\end{corollary}

In view of this corollary, it is justified to speak of ``\emph{the} splitting field of $p$ over $F$''. This field will be unique up to isomorphism.

\subsection{Group Characters}
\begin{definition}
	Let $G$ be a group and let $F$ be a field. A \defn{character}\index{group character} of $G$ in $F$ is a function $\sigma : G \to F$ such that
	\[
		\sigma(xy) = \sigma(x) \sigma(y)
	\]
	for all $x, y \in G$, and $\sigma(x) \neq 0$ for all $x \in G$.
	
	In other words, $\sigma$ is a (group) homomorphism from $G$ into the multiplicative group of the field $F$.
\end{definition}

\begin{definition}
	The characters $\sigma_1, \dots, \sigma_n$ are called \emph{dependent}\index{dependent (characters)} if there exists elements $a_1, \dots, a_n \in F$ (not all zero) such that
	\[
		a_1 \sigma_1(x) + a_2 \sigma_2(x) + \dots + a_n \sigma_n(x) = 0
	\]
	for all $x \in G$. The characters are \emph{independent}\index{independent (characters)} if they are not dependent.
\end{definition}

\begin{note}
	The set of all functions $\varphi : G \to F$. is a vector space uner pointwise addition
	\[
		(\varphi_1 + \varphi_2)(x) = \varphi_1(x) + \varphi_2(x) \quad \text{for all } x \in G
	\]
	and scalar multiplication given by
	\[
		(\alpha\varphi)(x) = \alpha\varphi(x) \quad \text{for all } x \in G, \text{ for all } \alpha \in F.
	\]
	The notion of dependence and independence coincide with `ordinary' linear dependence and independence in this vector space.
\end{note}

\begin{theorem}\label{thm:12}
	Let $G$ be a group and let $\sigma_1, \dots, \sigma_n$ be mutually distinct characters of $G$ in a field $F$. Then $\sigma_1, \dots, \sigma_n$ are independent.
	\begin{note}
		$\sigma_1$ and $\sigma_2$ are distinct if $\sigma_1(x) \neq \sigma_2(x)$ for some $x \in G$.
	\end{note}
	\begin{proof}
		We prove the result by induction on $n$.
		
		One character cannot be dependent since $a_1 \sigma_1(x) = 0$ for all $x \in G$ implies that $a_1 = 0$ (since $\sigma_1(x) \neq 0$ by definition).
		
		Now let $n > 1$. Suppose for a contradiction that
		\begin{equation}\label{eq:thm-12-non-trivial-dependence}
			a_1 \sigma_1(x) + a_2 \sigma_2(x) + \dots + a_n \sigma_n(x) = 0,
		\end{equation}
		for all $x \in G$, is a non-trivial dependence. Then all $a_i$ are non-zero, since otherwise we would have a non-trivial dependence between $n - 1$ (mutually distinct) characters, contradicting the induction hypothesis.
		
		Since $\sigma_1 \neq \sigma_n$, there exists an $\alpha \in G$ such that $\sigma_1(\alpha) \neq \sigma_n(\alpha)$. Multiplying the relation (\ref{eq:thm-12-non-trivial-dependence}) by $a_n^{-1}$ gives
		\begin{equation}\label{eq:hammer-and-sickle}
			b_1 \sigma_1(x) + b_2 \sigma_2(x) + \dots + \sigma_n(x) = 0,
		\end{equation}
		for all $x \in G$, where $b_i = a_n^{-1} a_i$, $i = 1, \dots, n - 1$. Replacing $x$ by $\alpha x$ in this relation gives
		\[
			b_1 \sigma_1(\alpha x) + b_2 \sigma_2(\alpha x) + \dots + \sigma_n(\alpha x) = 0,
		\]
		for all $x \in G$. Hence
		\[
			b_1 \sigma_1(\alpha) \sigma_1(x) + b_2 \sigma_2(\alpha) \sigma_2(x) + \dots + \sigma_n(\alpha) \sigma_n(x) = 0,
		\]
		for all $x \in G$. Now we multiply by $\sigma_n(\alpha)^{-1}$ (we can do this since $\sigma_n(\alpha) \neq 0$) to get
		\[
			\sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha) \sigma_1(x) + \dots + \underbrace{\sigma_n(\alpha)^{-1} \sigma_n(\alpha)}_{= 1} \sigma_n(x) = 0,
		\]
		for all $x \in G$. Subtracting this from (\ref{eq:hammer-and-sickle}) gives
		\begin{align}
			&[b_1 - \sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha)] \sigma_1(x) + \dots \nonumber \\
			& \quad \dots + [b_{n - 1} - \sigma_n(\alpha)^{-1} b_{n - 1} \sigma_{n - 1}(\alpha)] \sigma_{n - 1}(x) = 0, \label{eq:3}
		\end{align}
		for all $x \in G$. We now show that the coefficient at $\sigma_1(x)$ in this relation is not zero: if it is zero then we have
		\[
			b_1 - \sigma_n(\alpha)^{-1} b_1 \sigma_1(\alpha) = 0
		\]
		and so
		\[
			b_1 (1 - \sigma_n(\alpha)^{-1} \sigma_1(\alpha)) = 0.
		\]
		Since $b_1 \neq 0$, this gives
		\[
			1 - \sigma_n(\alpha)^{-1} \sigma_1(\alpha) = 0,
		\]
		and hence
		\[
			\sigma_1(\alpha) = \sigma_n(\alpha),
		\]
		a contradiction (to the choice of $\alpha$). Therefore the coefficient at $\sigma_1(x)$ is non-zero, so (\ref{eq:3}) a non-trivial dependence between $n - 1$ mutually distinct characters, contradicting the induction hypothesis.
		
		Hence any set of mutually distinct characters of $G$ in $F$ are independent.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:to-thm-12}
	Suppose $E$ and $E'$ are fields, and $\sigma_1, \dots, \sigma_n$ are mutually distinct monomorphisms from $E$ to $E'$. Then $\sigma_1, \dots, \sigma_n$ are independent.
	
	Here, independent means that there is no non-trivial relation of the form
	\[
		a_1 \sigma_1(x) + \dots + a_n \sigma_n(x) = 0
	\]
	for all $x \in E$, $a_1, \dots, a_n \in E'$ not all zero.
	\begin{proof}
		This follows immediately from Theorem \ref{thm:12} since $E \setminus \{0\}$ is a group under multiplication and the restrictions of the $\sigma_i$ to this group are mutually distinct characters from $E \setminus \{0\}$ to $E'$.
	\end{proof}
\end{corollary}

If $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ is a set of maps from $E$ to itself (i.e. for each $i$ we have $\varphi : E \to E$) then we say that $a \in E$ is a \emph{fixed point} for $\Sigma$ if $\varphi_i(a) = a$ for $i = 1, \dots, n$. It will be convenient to generalise this notion to maps between different sets.

\begin{definition}
	If $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ is a set of maps from $E$ to $E'$ we say that $a \in E$ is a \emph{fixed point}\index{fixed!point} for $\Sigma$ if
	\[
		\varphi_1(a) = \varphi_2(a) = \dots = \varphi_n(a).
	\]
\end{definition}

\begin{note}
	We get the ordinary definition of a fixed point if $E = E'$ and one of the maps $\varphi_i$ is the identity map.
\end{note}

\begin{lemma}
	Let $\Sigma = \{\varphi_1, \dots, \varphi_n\}$ be a set of monomorphisms from a field $E$ to a field $E'$. Then the set of fixed points for $\Sigma$ is a subfield of $E$.
	\begin{proof}
		If $a$ and $b$ are fixed points for $\Sigma$ then we have addition:
		\[
			\varphi_i(a + b) = \varphi_i(a) + \varphi_i(b) = \varphi_j(a) + \varphi_j(b) = \varphi_j(a + b);
		\]
		additive inverses:
		\[
			\varphi_i(-a) = -\varphi_i(a) = -\varphi_j(a) = \varphi_j(-a);
		\]
		and multiplication:
		\[
			\varphi_i(ab) = \varphi_i(a) \varphi_i(b) = \varphi_j(a) \varphi_j(b) = \varphi_j(ab).
		\]
		If $a \neq 0$ then we also have multiplicative inverses:
		\[
			\varphi_i(a^{-1}) = (\varphi_i(a))^{-1} = (\varphi_j(a))^{-1} = \varphi_j(a^{-1}).
		\]
		Therefore the set of fixed points is closed under the field operations, and therefore it is a subfield.
	\end{proof}
\end{lemma}

The subfield of fixed points for $\Sigma$ in $E$ is called the \emph{fixed subfield}\index{fixed!subfield} for $\Sigma$. We denote the fixed subfield for $\Sigma$ by $\Phi(\Sigma)$ \index{$\Phi(\Sigma)$|see{fixed subfield}}.

\begin{theorem}\label{thm:13}
	Suppose $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of mutually distinct monomorphisms from a field $E$ to a field $E'$, and let $F = \Phi(\Sigma)$ be the fixed field for $\Sigma$. Then $(E : F) \geq n$.
	\begin{proof}
		Suppose on the contrary that $(E : F) = r < n$. Let $\{\omega_1, \dots, \omega_r\}$ be a basis of $E$ over $F$, and consider the homogeneous system
		\begin{equation}\label{eq:thm-13-hom-sys}
			\sum_{i = 1}^n{\sigma_i(\omega_j)x_i} = 0 \qquad (j = 1, \dots, r)
		\end{equation}
		of linear equations in $E'$. Since the number $r$ of equations is less than the number of variables $n$, there exists a non-trivial solution, say $x_1, \dots, x_n \in E'$.
		
		Since $\{\omega_1, \dots, \omega_r\}$ is a basis of $E$, for any $\alpha \in E$ there exist elements $a_1, \dots, a_r \in F$ such that
		\[
			\alpha = a_1 \omega_1 + a_2 \omega_2 + \dots + a_r \omega_r.
		\]
		Now we multiply the $j$-th equation in (\ref{eq:thm-13-hom-sys}) by $\sigma_1(a_j)$ ($j = 1, \dots, r$). Since $a_j \in F$ and $F$ is the fixed subfield, we have
		\[
			\sigma_1(a_j) = \sigma_2(a_j) = \dots = \sigma_n(a_j) \qquad (j = 1, \dots, r)
		\]
		and since the $\sigma_i$s are monomorphisms, we have
		\[
			\sigma_i(a_j) \sigma_i(\omega_j) = \sigma(a_j \omega_j).
		\]
		Hence, the system (\ref{eq:thm-13-hom-sys}) turns into
		\[
			\begin{matrix}
				\sigma_1(a_1 \omega_1)x_1 & + & \dots & + & \sigma_n(a_1 \omega_1)x_n & = & 0 \\
				\vdots  &  & \vdots &  & \vdots &  & \vdots \\
				\sigma_1(a_r \omega_r)x_1 & + & \dots & + & \sigma_n(a_r \omega_r)x_n & = & 0.
			\end{matrix}
		\]
		Adding all these equations together and using
		\begin{align*}
			\sigma_i(a_1 \omega_1) + \sigma_i(a_2 \omega_2) + \dots + \sigma_i(a_r \omega_r) &= \sigma_i(a_1 \omega_1 + a_2 \omega_2 + \dots + a_r \omega_r) \\
				&= \sigma_i(\alpha),
		\end{align*}
		we get
		\[
			\sigma_1(\alpha)x_1 + \sigma_2(\alpha)x_2 + \dots + \sigma_n(\alpha)x_n = 0
		\]
		for all $\alpha \in E$. This is a non-trivial relation between $n$ mutually distinct monomorphisms, a contradiction to Corollary \ref{cor:to-thm-12}.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:to-thm-13}
	If $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of mutually distinct automorphisms of a field $E$ and $F = \Phi(\Sigma)$ is the fixed subfield for $\Sigma$, then $(E : F) \geq n$.
	\begin{proof}
		Immediate from Theorem \ref{thm:13} (with $E' = E$).
	\end{proof}
\end{corollary}

\begin{definition}
	If $F$ is a subfield of a field $E$, and $\sigma$ is an automorphism of $E$, we say that \emph{$\sigma$ leaves $F$ fixed}\index{$\sigma$ leaves $F$ fixed} (or \emph{$\sigma$ fixes $F$}) if $\sigma(a) = a$ for all $a \in F$.
\end{definition}

\begin{proposition}
	The set of all automorphisms of $E$ is a group under composition of maps.
	\begin{proof}
		Let's use the definition of a group.
		\begin{enumerate}
			\item Let $\sigma, \tau$ be automorphisms of $E$. Then for all $a, b \in E$ we have
			\begin{align*}
				\sigma\tau(a + b) &= \sigma(\tau(a + b)) = \sigma(\tau(a) + \tau(b)) \\
					&= \sigma(\tau(a)) + \sigma(\tau(b)) = \sigma\tau(a) + \sigma\tau(b),
			\end{align*}
			and similarly
			\[
				\sigma\tau(ab) = \sigma\tau(a)\sigma\tau(b).
			\]
			Since the composition of bijections is still a bijection, it follows that the set of all automorphisms of $E$ is closed under composition.
			\item The \emph{identity map} on $E$ is the identity element of the group of all automorphisms.
			\item We showed in Example sheet IV, Question 2, that the \emph{inverse} map of an automorphism is also an automorphism.
			\item \emph{Associativity} follows by the fact that composition of functions is associative.
		\end{enumerate}
	\end{proof}
\end{proposition}

\begin{notation}
	$\Gamma(E) = \Aut(E) =$ the group of automorphisms on $E$.\index{$\Gamma(E)$}
\end{notation}

\begin{lemma}
	If $E$ is an extension field of $F$, then the set $\Gamma(E : F)$\index{$\Gamma(E : F)$} of automorphisms of $E$ that fix $F$ is a subgroup of $\Gamma(E)$.
	\begin{proof}
		Let's use the subgroup criterion.
		\begin{enumerate}
			\item Let $\sigma, \tau \in \Gamma(E : F)$, let $a \in F$. Then
			\[
				\sigma\tau(a) = \sigma(\tau(a)) = \sigma(a) = a.
			\]
			So $\sigma\tau \in \Gamma(E : F)$.
			\item Clearly, the identity map of $E$ fixes $F$, so $\text{id}|_E \in \Gamma(E : F)$.
			\item Let $\sigma \in \Gamma(E : F)$ and $a \in F$. Then
			\[
				\sigma^{-1}(a) = \sigma^{-1}(\sigma(a)) = \sigma^{-1}\sigma(a) = a.
			\]
		\end{enumerate}
		Hence $\Gamma(E : F) \leq \Gamma(E)$.
	\end{proof}
\end{lemma}

\begin{definition}
	\emph{$\Gamma(E : F)$}\index{$\Gamma(E : F)$} is called the \emph{Galois group}\index{Galois group!of an extension} of the extension $E : F$.
\end{definition}

\begin{remark}
	All automorphisms of $\Gamma(E : F)$ leave the field $F$ fixed. However, this does not mean that $F$ is the fixed field of $\Gamma(E : F)$; the fixed field may in fact be larger.
\end{remark}

\begin{example}
	Let $E = \Q(\alpha)$, where $\alpha = \sqrt[3]{2}$ is the real cube root of $2$. Then
	\[
		E = \{a + b\alpha + c\alpha^2 \mid a, b, c \in \Q\}.
	\]
	Now let $\sigma \in \Gamma(E : \Q)$. Then applying $\sigma$ to $\alpha^3 = 2$ yields
	\[
		[\sigma(\alpha)]^3 = \sigma(\alpha^3) = \sigma(2) = 2.
	\]
	So $\sigma(\alpha)$ is a cube root of 2. Since $E \subset \R$ and $\alpha$ is the only real number such that $\alpha^3 = 2$, we must have that $\sigma(\alpha) = \alpha$. But then
	\begin{align*}
		\sigma(a + b\alpha + c\alpha^2) &= \sigma(a) + \sigma(b)\sigma(\alpha) + \sigma(c)\sigma(\alpha)^2 \\
			&= a + b\alpha + c\alpha^2,
	\end{align*}
	for all $a, b, c \in \Q$. Hence the only $\Q$-automorphism of $E$ is the identity map. Therefore $\Gamma(E : \Q) = \{1\}$. But, of course, the identity fixes all of $E$, so $\Phi(\Gamma(E : F)) = E$.
\end{example}

A special class of extensions $E : F$ are the ones where $\Phi(\Gamma(E : F)) = F$. (The Main Theorem of Galois Theory is about such extensions.)

\begin{definition}
	An extension $E : F$ is called \emph{normal}\index{normal extension} if $F$ is the fixed field of $\Gamma(E : F)$ and $(E : F) < +\infty$.
\end{definition}

Recall that if $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a set of $n$ automorphisms of $E$, and $F$ is the fixed field of $\Sigma$, then $(E : F) \geq n$ (Corollary \ref{cor:to-thm-13}). If the automorphisms in $\Sigma$ form a group, we can say more.

\begin{theorem}\label{thm:14}
	If $\Sigma = \{\sigma_1, \dots, \sigma_n\}$ is a finite \emph{group} of $n$ automorphisms of a field $E$, and $F$ is the fixed subfield in $E$, then $(E : F) = n$.
	\begin{proof}
		Since $\Sigma$ is a group, it contains the identity automorphism, say $\sigma_1 = \text{id}|_E$.
		
		Suppose for a contradiction that $(E : F) > n$. Then there exist $n + 1$ elements in $E$ that are linearly independent over $F$, say $\sigma_1, \dots, \sigma_{n + 1}$. Consider the homogeneous system
		\begin{equation}\label{eq:thm-14-hom-sys}
			\begin{matrix}
				x_1 \sigma_1(\alpha_1) & + & x_2 \sigma_1(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_1(\alpha_{n + 1}) & = & 0 \\
				x_1 \sigma_2(\alpha_1) & + & x_2 \sigma_2(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_2(\alpha_{n + 1}) & = & 0 \\
				\vdots &  & \vdots &  & \vdots &  & \vdots &  & \vdots \\
				x_1 \sigma_n(\alpha_1) & + & x_2 \sigma_n(\alpha_2) & + & \dots & + & x_{n + 1} \sigma_n(\alpha_{n + 1}) & = & 0.
			\end{matrix}
		\end{equation}
		Since there are more indeterminants than equations, the system has a non-trivial solution, say $x_1, \dots, x_{n + 1} \in E$. Observe that this solution cannot lie entirely in $F$, otherwise the first equation in (\ref{eq:thm-14-hom-sys}) would give (using $\sigma_1 = \text{id}|_E$)
		\[
			x_1 \alpha_1 + x_2 \alpha_2 + \dots + x_{n + 1} \alpha_{n + 1} = 0.
		\]
		But the $\alpha_i$s are linearly independent over $F$, so this is not possible.
		
		Among the solutions of (\ref{eq:thm-14-hom-sys}), we choose one that has the fewest non-zero elements. Without loss of generality, we may assume that this is
		\[
			a_1, a_2, \dots, a_r, 0, 0, \dots, 0,
		\]
		with the first $r$ of them non-zero. Note that we must have $r > 1$, since otherwise
		\[
			a_1 \sigma_1(\alpha_1) = a_1 \alpha_1 = 0,
		\]
		which gives that $a_1 = 0$ -- but our solution is non-trivial.
		
		We may also assume that $a_r = 1$ by multiplying the solution by $a_r^{-1}$ if necessary. Thus our system now looks like this:
		\begin{equation}\label{eq:orthodox-cross}
			a_1 \sigma_i(\alpha_1) + a_2 \sigma_i(\alpha_2) + \dots + \sigma_i(\alpha_r) = 0 \qquad (i = 1, \dots, n).
		\end{equation}
		Since at least one of the $a_i$s must lie outside of $F$, we may assume that $a_1 \in E \setminus F$. Hence there is an automorphism $\sigma_k \in \Sigma$ such that $\sigma_k(a_1) \neq a_1$. Since $\Sigma$ is a group, we have that $\Sigma = \{\sigma_k\sigma_1, \sigma_k\sigma_2, \dots, \sigma_k\sigma_n\}$. By applying $\sigma_k$ to all the equations in (\ref{eq:orthodox-cross}) we get
		\[
			\sigma_k(a_1) \sigma_k\sigma_j(\alpha_1) + \sigma_k(a_2) \sigma_k\sigma_j(\alpha_2) + \dots + \sigma_k\sigma_j(\alpha_r) = 0 \qquad (j = 1, \dots, n).
		\]
		By setting $\sigma_k\sigma_j = \sigma_i$ this becomes
		\begin{equation}\label{eq:soviet-star}
			\sigma_k(a_1) \sigma_i(\alpha_1) + \sigma_k(a_2) \sigma_i(\alpha_2) + \dots + \sigma_i(\alpha_r) = 0 \qquad (i = 1, \dots, n).
		\end{equation}
		Subtracting (\ref{eq:soviet-star}) from (\ref{eq:orthodox-cross}) gives
		\[
			[a_1 - \sigma_k(a_1)] \sigma_i(\alpha_1) + \dots + [a_{r - 1} - \sigma_k(a_{r - 1})]\sigma_i(\alpha_{r - 1}) = 0 \qquad (i = 1, \dots, n).
		\]
		In particular, note that the leading term $a_1 - \sigma_k(a_1) \neq 0$, so this is a non-trivial solution of the original system (\ref{eq:thm-14-hom-sys}) with fewer than $r$ non-zero elements, a contradiction.
		
		Hence $(E : F) = n$.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:1}
	If $F$ is a fixed field for a finite group $G$ of automorphisms of $E$, then each automorphism of $E$ that fixes $F$ must belong to $G$.
	\begin{proof}
		By Theorem \ref{thm:14} we have $(E : F) = |G| = n$.
		
		Assume that there is an automorphism $\sigma$ that fixes $F$ but $\sigma \notin G$. Then $F$ would be fixed by $n + 1$ (mutually distinct) automorphisms, namely $\sigma$ and the $n$ automorphisms in $G$. But then Corollary \ref{cor:to-thm-13} gives that $(E : F) \geq n + 1$, a contradiction.
	\end{proof}
\end{corollary}

\begin{corollary}\label{cor:2}
	Distinct finite subgroups of $\Gamma(E)$ have distinct fixed fields.
	\begin{proof}
		Follows immediately from Corollary \ref{cor:1}.
	\end{proof}
\end{corollary}

\begin{corollary}\label{cor:3}
	If $E : F$ is a normal extension with Galois group $\Gamma(E : F)$, then
	\[
		|\Gamma(E : F)| = (E : F).
	\]
	\begin{proof}
		If $E : F$ is normal, then $(E : F)$ is finite and $F$ is the fixed field for $\Gamma(E : F)$. Hence $|\Gamma(E : F)| = (E : F)$.
	\end{proof}
\end{corollary}

\begin{definition}
	A \emph{polynomial} $f \in F[x]$ is \emph{separable}\index{separable!polynomial} if the irreducible factors of $f$ have no repeated roots.
	
	An \emph{element} $\alpha$ in an extension $E$ of $F$ is called \emph{separable over $F$}\index{separable!element over $F$} if it is a root of a separable polynomial in $F[x]$.
	
	An \emph{extension} $E : F$ is called \emph{separable}\index{separable!extension} if each element of $E$ is separable over $F$.
\end{definition}

\begin{example}[A non-separable polynomial]
	Let $F = \Z_2$ and let $E = \Z_2(t)$ be the field of rational functions over $F$. Then
	\[
		f = x^2 + t \in E[x],
	\]
	where $t$ is a linear rational function, is not separable over $E$.
	\begin{proof}
		Let $\alpha$ be a zero of $f$. Then $\alpha^2 = -t = t$ (as we are in characteristic 2). Hence
		\[
			(x - \alpha)^2 = x^2 - 2\alpha x + \alpha^2 = x^2 + t,
		\]
		and hence $\alpha$ is a root of multiplicity 2.
		
		It remains to be shown that $f$ is irreducible. Suppose $f$ is reducible over $E$. Then $f$ would be the product of 2 linear factors, so it would have a root in $E$. Now if $g, h \in \Z_2[t]$ with $h \neq 0$ and $\frac{g}{h} \in E$ is a root of $f$, then
		\[
			\left(\frac{g}{h}\right)^2 = t
		\]
		and therefore $g^2 = th^2$. However this is not possible because $g^2$ is a polynomial of even degree and $th^2$ is a polynomial of odd degree in $\Z_2[t]$.
		
		Hence $f$ is irreducible and thus not separable over $E$.
	\end{proof}
\end{example}

We now introduce a useful tool for deciding whether a polynomial has multiple roots.

\begin{definition}
	Let $F$ be a field. The map $D: F[x] \to F[x]$ defined by
	\[
		D(a_0 + a_1 x + \dots + a_n x^n) = a_1 + 2a_2 x + \dots + nx^{n - 1}
	\]
	is called the \defn{formal derivative} on $F[x]$.
\end{definition}

\begin{remark}
	The distinct difference between this definition and the usual derivative (on polynomials) is that we do not rely on the notion of \emph{limits} here.
\end{remark}

\begin{proposition}[Properties of the formal derivative $D$]
	For $f, g \in F[x]$ we have:
	\begin{enumerate}
		\item $D(f + g) = D(f) + D(g)$.
		\item $D(fg) = D(f)g + fD(g)$.
		\item $D(f^n) = nf^{n - 1} D(f)$.
	\end{enumerate}
	\begin{proof}
		Exercise.
		% I started writing it up and then thought, ``Screw this!''
		\begin{comment}
		Suppose that $\deg{f} = n \leq m = \deg{g}$ and that $f = a_0 + \dots + a_n x^n$ and $g = b_0 + \dots + b_m x^m$.
		\begin{enumerate}
			\item We have
			\begin{align*}
				D(f + g) &= D((a_0 + b_0) + \dots + (a_n + b_n) x^n \\
						& \qquad + b_{n + 1}x^{n + 1} + \dots + b_m x^m) \\
					&= 2(a_1 + b_1) + \dots + n(a_n + b_n) x^{n - 1} \\
						& \qquad + (n + 1)b_{n + 1}x^n + \dots + m b_m x^{m - 1} \\
					&= 2a_1 + \dots + n a_n x^{n - 1} + 2b_1 + \dots + + m b_m x^{m - 1} \\
					&= D(f) + D(g).
			\end{align*}
			\item We have
			\begin{align*}
				D(fg) &=  \\
					&= D(f)g + fD(g).
			\end{align*}
			\item We have
			\begin{align*}
			D(f^n) &=  \\
				&= nf^{n - 1} D(f).
			\end{align*}
		\end{enumerate}
		\end{comment}
	\end{proof}
\end{proposition}

\begin{theorem}\label{thm:19}
	A polynomial $f \in F[x]$ has multiple roots in a splitting field $E$ of $f$ if and only if $f$ and $Df$ have a common root in $E$.
	
	The latter is equivalent to $f$ and $Df$ having a common factor of degree greater than or equal to $1$ in $F[x]$.
	\begin{proof}
		If $\alpha$ is a multiple root of $f$, then
		\[
			f = (x - \alpha)^2 g
		\]
		in $E[x]$ for some polynomial $g \in E[x]$. This gives
		\[
			Df = 2(x - \alpha)g + (x - \alpha) Dg.
		\]
		Hence $(x - \alpha)$ divides $Df$ and so $f$ and $Df$ have a common root.
		
		Conversely, if $\alpha$ is a common root of $f$ and $Df$, then
		\[
			f = (x - \alpha)g \quad \text{and} \quad Df = (x - \alpha)h
		\]
		for some $g, h \in E[x]$. So we have
		\[
			Df = g + (x - \alpha)Dg = (x - \alpha)h.
		\]
		Hence $(x - \alpha) \mid g$, so this gives that $(x - \alpha)^2 \mid f$.
		
		Finally we prove the final statement in the theorem. If $f$ and $Df$ have a common factor $q$, then any root of $q$ will be common to $f$ and $Df$. If $f$ and $Df$ have a common root $\alpha$, then the minimum polynomial of $\alpha$ will be a common factor of $f$ and $Df$.
	\end{proof}
\end{theorem}

\begin{corollary}\label{cor:to-thm-19}
	If $F$ is a field of characteristic 0, then any irreducible polynomial in $F[x]$ is separable.
	\begin{proof}
		If $f \in F[x]$ is irreducible, then it does not have any factors of smaller degree (other than constants). Moreover, if $f$ is non-constant then $Df \neq 0$. Hence $f$ and $Df$ have no non-constant common factors. Hence, by Theorem \ref{thm:19}, $f$ has no multiple roots.
	\end{proof}
\end{corollary}

We will use the following Lemma for the proof of the subsequent Theorem.
\begin{lemma}\label{lem:normal-extensions-separable}
	Any normal extension $E : F$ is separable. Moreover, any element of $E$ is a root of a polynomial that splits completely in $E$.
	\begin{proof}
		Let $G = \Gamma(E : F) = \{\sigma_1, \dots, \sigma_s\}$ with $F = \Phi(G)$. Let $\alpha \in E$ and let $\{\alpha_1, \dots, \alpha_r\}$ be the set of distinct elements in the sequence $\sigma_1(\alpha), \dots, \sigma_s(\alpha)$. Since $G$ is a group,
		\[
			\sigma_j(\alpha_i) = \sigma_j(\sigma_k(\alpha)) = (\sigma_j \sigma_k)(\alpha) = \sigma_m(\alpha) = \alpha_n,
		\]
		for some $k, m, n$. Hence the elements $\{\alpha_1, \dots, \alpha_r\}$ are permuted by the automorphisms of $G$. It follows that the polynomial
		\[
			f = (x - \alpha_1)(x - \alpha_2)\dots(x - \alpha_r)
		\]
		is fixed by the automorphisms of $G$ (since the automorphisms merely permute the order of the linear factors on the right-hand side). By assumption, the only elements in $E$ that are fixed by all automorphisms in $G$ are the elements of $F$, and hence $f \in F[x]$. Moreover, $f$ is a separable polynomial, as it has $r$ distinct roots $\alpha_1, \dots, \alpha_r$, and $\alpha$ is one of them. Also, $f$ splits completely in $E$.
	\end{proof}
\end{lemma}

\begin{theorem}\label{thm:15}
	An extension $E : F$ is normal if and only if $E$ is a splitting field of a separable polynomial $p \in F[x]$.
	\begin{proof}
		($\impliedby$) Suppose $E$ is a splitting field of a separable polynomial $p \in F[x]$. By the definition of a normal extension, we need to show that $F = \Phi(\Gamma(E : F))$.
		
		If all roots of $p$ are in $F$, then $E = F$ and $\Gamma(E : F) = \{1\}$, so the assertion is trivially true.
		
		Now suppose $p$ has $n > 1$ roots in $E \setminus F$. We proceed by induction on $n$, assuming $E$ is normal over $F$ whenever $p$ has fewer than $n$ roots outside $F$.
		
		Let $p = p_1 p_2 \dots p_r$ be a factorisation of $p$ into irreducibles. Then at least one of these irreducibles has degree $> 1$, since otherwise all roots would be in $F$. Suppose $\deg{p_1} = s > 1$ and let $\alpha_1$ be a root of $p_1$. Consider the simple extension $F(\alpha_1)$. Then $(F(\alpha_1) : F) = \deg{p_1} = s > 1$. If we now consider $F(\alpha_1)$ as the new ground field, then fewer than $n$ roots of $p$ will be outside $F(\alpha)$. Then the inductive hypthesis gives that $E$ is a normal extension of $F(\alpha_1)$. In other words, each element of $E \setminus F(\alpha_1)$ is moved by at least one automorphism of $E$ that leaves $F(\alpha_1)$ fixed.
		
		Since $p$ is separable, the roots $\alpha_1, \alpha_2, \dots, \alpha_s$ of $p_1$ are distinct elements of $E$. By Theorem \ref{thm:8}, there exist isomorphisms $\sigma_1, \dots, \sigma_s$ mapping $F(\alpha_1)$ onto $F(\alpha_1), F(\alpha_2), \dots, F(\alpha_s)$, respectively, mapping $\alpha_1$ to $\alpha_1, \alpha_2, \dots, \alpha_s$, respectively, and leaving $F$ fixed.\footnote{(Refer to Theorem \ref{thm:8}) We use $F' = F$, which gives that $p = f' = f$ ($f, f'$ from the Theorem) and so we have $E = F(\alpha_1)$ and $E'(\alpha_i)$ (since $f'(\alpha_i) = p(\alpha_i) = 0$). We can then extend any automorphism $\sigma : F \to F$ which fixes $F$ to $\sigma_i : F(\alpha_1) \to F(\alpha_i)$ such that $\sigma_i(\alpha_1) = \alpha_i$.}
		
		Now we apply Theorem \ref{thm:10}. Since $E$ is a splitting field of $p$ over $F$, it is also a splitting field of $p$ over each of the $F(\alpha_i)$, $i = 1, \dots, s$. The isomorphisms $\sigma_1, \sigma_2, \dots, \sigma_s$ leave $p$ fixed, and hence by Theorem \ref{thm:10}, they can be extended to automorphisms of $E$. Denote these by $\sigma_1, \sigma_2, \dots, \sigma_s$ again.\footnote{We are simply overloading the notation instead of denoting them $\sigma'_i$, which could get a bit messy.} In short, the application of Theorems \ref{thm:8} and \ref{thm:10} yields automorphisms $\sigma_1, \dots, \sigma_s$ of $E$ that fix $F$ and map $\alpha_1$ to $\alpha_1, \dots, \alpha_s$, respectively.
		
		Now let $\theta$ be an element of $E$ that is fixed by all automorphisms of $\Gamma(E : F)$, i.e. $\theta \in \Phi(\Gamma(E : F))$. We know that $\theta \in F(\alpha_1)$ (because each element of $E \setminus F(\alpha_1)$ is moved by at least one automorphism of $E$ that leaves $F(\alpha_1)$, and hence $F$, fixed). Hence $\theta$ is of the form
		\[
			\theta = c_0 + c_1 \alpha_1 + \dots + c_{s - 1} \alpha^{s - 1},
		\]
		where $c_0, \dots, c_{s - 1} \in F$. By applying $\sigma_i$ to this equation we get (since $\sigma_i(\theta) = \theta$)
		\[
			\theta = c_0 + c_1 \alpha_i + \dots c_{s - 1} \alpha_i^{s - 1}.
		\]
		Consequently,
		\[
			c_0 - \theta + c_1 \alpha_i + \dots c_{s - 1} \alpha_i^{s - 1} = 0,
		\]
		for $i = 1, \dots, s$. Hence the polynomial
		\[
			c_0 - \theta + c_1 x + \dots c_{s - 1} x^{s - 1}
		\]
		has zeros $\alpha_1, \dots, \alpha_s$, that is, $s$ \emph{distinct} zeros. But the degree of this polynomial is $s - 1$ and therefore it must be the zero polynomial, i.e. all the coefficients are 0. In particular, $c_0 - \theta = 0$ and so $\theta = c_0 \in F$. So $F$ is the fixed field of $\Gamma(E : F)$, i.e. $E : F$ is a normal extension.
		\vspace{2ex}
		
		\noindent ($\implies$) Let $\{\omega_1, \dots, \omega_r\}$ be a basis of $E$ as a vector space over $F$.
		
		Since $E : F$ is normal, Lemma \ref{lem:normal-extensions-separable} gives that there exist separable polynomials $f_1, \dots, f_t \in F[x]$ with $\omega_1, \dots, \omega_t$, respectively, as a root. Moreover, $f_1, \dots, f_t$ split in $E$. Then $E$ is the splitting field of $p = f_1 f_2 \dots f_t$.
	\end{proof}
\end{theorem}

\begin{corollary}
	In characteristic zero, $E : F$ is normal if and only if $E$ is a splitting field for a polynomial in $F[x]$.
	\begin{proof}
		In characteristic zero, all polynomials are separable.
	\end{proof}
\end{corollary}

We have come to the first climax of the course, the Fundamental Theorem of Galois Theory. We first present the following result which will be used in the final part of the proof of the Fundamental Theorem.
\begin{lemma}\label{lem:used-in-ftogt-iii}
	Let $B$ be an intermediate field of $E : F$, and let
	\[
		H = \Gamma(E : B) \leq G = \Gamma(E : F).
	\]
	Then for all $\sigma \in G$,
	\[
	\Gamma(E : \sigma(B)) = \sigma H \sigma^{-1}.
	\]
	\begin{proof}
		Let $\tau \in H$ and let $b' \in \sigma(B)$, i.e. $b' = \sigma(b)$ for some $b \in B$. Then
		\begin{align*}
			\sigma \tau \sigma^{-1}(b') &= \sigma \tau \sigma^{-1} \sigma(b) \\
				&= \sigma \tau(b) \\
				&= \sigma(b) \qquad \text{(since } \tau \text{ fixes B)} \\
				&= b'.
		\end{align*}
		Hence $\sigma \tau \sigma^{-1}$ fixes $\sigma(B)$ and therefore $\Gamma(E : \sigma(B)) \supseteq \sigma H \sigma^{-1}$.
		
		Conversely, suppose $\tau' \in \Gamma(E : \sigma(B))$, i.e. $\tau'(b') = b'$ for all $b' \in \sigma(B)$ as above. Then for all $b \in B$, we have $\tau' \sigma(b) = \sigma(b)$ and hence $\sigma^{-1} \tau' \sigma(b) = b$. But this means that $\sigma^{-1} \tau' \sigma \in H$, that is, $\sigma^{-1} \tau' \sigma = \tau$ for some $\tau \in H$. But then
		\[
			\tau' = \sigma \tau \sigma^{-1} \in \sigma H \sigma^{-1}.
		\]
		Hence $\Gamma(E : \sigma(B)) \subseteq \sigma H \sigma^{-1}$, and thus $\Gamma(E : \sigma(B)) = \sigma H \sigma^{-1}$.
	\end{proof}
\end{lemma}

\begin{remark}
	Such (relatively) short proofs are examinable. Longer proofs such as the following are probably not examinable.
\end{remark}

\begin{theorem}[The Fundamental Theorem of Galois Theory]\index{Fundamental Theorem of Galois Theory}
	Let $E$ be a normal extension of a field $F$ with Galois group $G = \Gamma(E : F)$. Then:
	\begin{enumerate}[(i)]
		\item There is a one-to-one correspondence between the set $\mathcal{H}$ of all subgroups of $G$ and the set $\mathcal{B}$ of all intermediate fields of $E : F$ given by $H \to \Phi(H)$, $H \in \mathcal{H}$, with inverse $B \to \Gamma(E : B)$, $B \in \mathcal{B}$.
		\item For each intermediate field $B \in \mathcal{B}$ we have
		\[
			(E : B) = |\Gamma(E : B)| \quad \text{and} \quad (B : F) = [G : \Gamma(E : B)].\footnote{Recall: If $H$ is a group and $K \leq H$, then $[H : K]$ denotes the index of $K$ in $H$, the number of cosets of $K$ in $H$.}
		\]
		In particular, $|G| = (E : F)$.
		\item An intermediate field $B$ is a normal extension of $F$ if and only if $\Gamma(E : B)$ is a normal subgroup of $G$. In this case,
		\[
			\Gamma(B : F) \cong G / \Gamma(E : B).
		\]
	\end{enumerate}
	\begin{proof}\hfill
		\begin{enumerate}[(i)]
			\item Since $E : F$ is a normal extension, Theorem \ref{thm:15} gives that $E$ is a splitting field of the separable polynomial $p \in F[x]$. For any intermediate field $B$, $p$ may be regarded as a polynomial in $B[x]$ and then $E$ is still a splitting field for $p$. Hence $E : B$ is a normal extension for all intermediate fields $B \in \B$. Hence $B = \Phi(H)$, where $H = \Gamma(E : B)$ (because the fixed of an extension field is exactly the base field). This means that the map $H \to \Phi(H)$ is onto.
			
			Moreover, by Corollary \ref{cor:2}, distinct subgroups of the Galois group have distinct fixed fields. Hence the map $H \to \Phi(H)$ is also injective and hence a bijection.
			\item Now let $B$ be an intermediate field of $E : F$. Then, as shown in the proof of (i), $B$ is the fixed field for the group $\Gamma(E : B)$, and by Theorem \ref{thm:14} we have $(E : B) = |\Gamma(E : B)|$. In particular, we have
			\[
				(E : F) = |\Gamma(E : F)| = |G|.
			\]
			By the Tower Law, we have $(E : F) = (E : B)(B : F)$, so
			\begin{align*}
				(B : F) &= (E : F) / (E : B) \\
					&= |\Gamma(E : F)| / |\Gamma(E : B)| \\
					&= |G| / |\Gamma(E : B)| \\
					&= [G : \Gamma(E : B)]
			\end{align*}
			by Lagrange's Theorem.\footnote{Lagrange's Theorem: If $G$ is a group and $H \leq G$, then $|G| = [G : H] \cdot |H|$.}
			\item Suppose an intermediate field $B$ is a normal extension of $F$.
			
			Claim: For every $\sigma \in G$ we have $\sigma(B) = B$. (Note that elements in $B$ can be permuted by $\sigma$, i.e. $b \in B$ does \emph{not} necessarily mean $\sigma(b) = b$.) Indeed, by Theorem \ref{thm:15}, $B$ is the splitting field of some (separable) polynomial $p \in F[x]$. Hence $B$ is generated by $F$ and the roots of $p$ (see Exercises IV). But $\sigma$ fixes $F$ and maps each root of $p$ to some root of $p$. So $\sigma(B)$ is also generated by $F$ and the roots of $p$ and hence $\sigma(B) = B$.
			
			Now, Lemma \ref{lem:used-in-ftogt-iii} gives
			\[
				H = \Gamma(E : B) = \Gamma(E : \sigma(B)) = \sigma H \sigma^{-1},
			\]
			and hence $H$ is a normal subgroup of $G$ (that's the definition of a normal subgroup!).
			
			On the other hand, if $H$ is normal in $G$, then for all $\sigma \in G$,
			\[
				\Gamma(E : \sigma(B)) = \sigma H \sigma^{-1} = H = \Gamma(E : B).
			\]
			Hence by (i), $\sigma(B) = B$.
			
			Since $\sigma(B) = B$, each element of $G = \Gamma(E : F)$ induces an automorphism of $B$ via $\sigma \mapsto \sigma|_B$, giving rise to a homomorphism $\psi : G \to \Gamma(B : F)$. \emph{Clearly}, $\ker{\psi} = \Gamma(E : B)$.\footnote{In all seriousness: The kernel of the \emph{group} homomorphism $\psi$  the set of elements in $G$ which are are mapped to the \emph{identity} element of $\Gamma(B : F)$. Therefore, it really is blindingly obvious that $\Gamma(E : B)$ is the kernel!} Therefore by the First Isomorphism Theorem, $\text{Im}{\psi} \cong G / \Gamma(E : B)$. By (ii), we have
			\[
				|\text{Im}{\psi}| = [G : \Gamma(E : B)] = (B : F).
			\]
			By Theorem \ref{thm:13}, $|\Gamma(B : F)| \leq (B : F)$. Since $\text{Im}{\psi} \subseteq \Gamma(E : B)$, it follows that $\text{Im}{\psi} = \Gamma(B : F)$, and hence $|\Gamma(B : F)| = (B : F)$.
			
			Now consider the fixed field $D := \Phi(\Gamma(B : F))$. By Theorem \ref{thm:14}, the degree of $B$ over $D$ is
			\[
				(B : D) = |\Gamma(B : F)| = (B : F).
			\]
			But then the Tower Law applied to $F \subseteq D \subseteq B$ gives that $(D : F) = 1$. Thererfore $D = F$ and hence $B : F$ is a normal extension (i.e. the fixed field is exactly $F$.)
			
			Finally, we have already proved that $\Gamma(B : F) = \text{Im}{\psi} \cong G / \Gamma(E : B)$.
		\end{enumerate}
	\end{proof}
\end{theorem}

\subsection{Galois Groups of Polynomials}
\begin{definition}
	If $F$ is a field and $f \in F[x]$, we define the \emph{Galois group of the polynomial} $f$ as
	\[
		\Gamma(f) = \Gamma(f, F) = \Gamma(E : F),
	\]
	\index{$\Gamma(f)$}\index{$\Gamma(f, F)$}\index{Galois group!of a polynomial}
	where $E$ is the splitting field of $f$ over $F$.
\end{definition}

The Fundamental Theorem for $\Gamma(f)$ is the same as for $\Gamma(E : F)$, with only a minor restatement in the assumptions:
\begin{theorem}\index{Fundamental Theorem of Galois Theory!for polynomials}
	If $f$ is a separable polynomial with coefficients in a field $F$ and $G = \Gamma(f) = \Gamma(E : F)$, where $E$ is a splitting field of $f$ over $F$, then
	\begin{enumerate}[(i)]
		\item There is a one-to-one correspondence between the set $\mathcal{H}$ of all subgroups of $G$ and the set $\mathcal{B}$ of all intermediate fields of $E : F$ given by $H \to \Phi(H)$, $H \in \mathcal{H}$, with inverse $B \to \Gamma(E : B)$, $B \in \mathcal{B}$.
		\item For each intermediate field $B \in \mathcal{B}$ we have
		\[
			(E : B) = |\Gamma(E : B)| \quad \text{and} \quad (B : F) = [G : \Gamma(E : B)].\footnote{Recall: If $H$ is a group and $K \leq H$, then $[H : K]$ denotes the index of $K$ in $H$, the number of cosets of $K$ in $H$.}
		\]
		In particular, $|G| = (E : F)$.
		\item An intermediate field $B$ is a normal extension of $F$ if and only if $\Gamma(E : B)$ is a normal subgroup of $G$. In this case,
		\[
			\Gamma(B : F) \cong G / \Gamma(E : B).
		\]
	\end{enumerate}
\end{theorem}

\begin{remark}
	The one-to-one correspondence between subgroups $\mathcal{H}$ of the Galois group and intermediate fields $\mathcal{B}$ (called the \defn{Galois correspondence} for $E : F$) is inclusion reversing:
	\[
		H_1 \leq H_2 \implies \Phi(H_1) \supseteq \Phi(H_2)
	\]
	and
	\[
		B_1 \subseteq B_2 \implies \Gamma(E : B_1) \geq \Gamma(E : B_2).
	\]
\end{remark}

\subsection{Some Worked Examples}
\begin{example}
	Let $E$ be the splitting field of $f = (x^2 - 2)(x^2 - 3)$ over $\Q$, i.e. $E = \Q(\sqrt{2}, \sqrt{3})$. We know that $(E : \Q) = 4$ (because $x^2 - 2$ is the minimum polynomial for $\sqrt{2}$ over $\Q$, and $x^2 - 3$ is the minimum polynomial for $\sqrt{3}$ over $\Q(\sqrt{2})$ -- see Exercises). Since $f \in \Q[x]$ and $\Q$ has characteristic 0, Corollary \ref{cor:to-thm-19} says that $f$ is separable (in fact, it is easy to see that $f$ has no repeated roots). Since $E$ is a splitting field for $f$, it follows from Theorem \ref{thm:15} that $E : \Q$ is a normal extension.\footnote{In short, $(E : \Q)$ is a normal extension because $E$ is a splitting field for $f$ in characteristic 0.} Therefore the Fundamental Theorem of Galois Theory applies! In particular, if $G = \Gamma(E : \Q)$, then $|G| = 4$ ($G$ is a group of order 4).
	
	We know that any automorphism $\sigma \in \Gamma(E : \Q)$ is completely determined by the effect of $\sigma$ on the generators $\sqrt{2}$, $\sqrt{3}$. These satisfy $x^2 - 2$ and $x^2 - 3$, respectively. Hence the possible images are $\sqrt{2} \mapsto \pm\sqrt{2}$ and $\sqrt{3} \mapsto \pm\sqrt{3}$, i.e. there are precisely 4 images for the pair $\sqrt{2}$, $\sqrt{3}$, as follows:
	\begin{center}
		\begin{tabular}{r l l}
		$\text{id} =\sigma_1$ : & $\sqrt{2} \mapsto \sqrt{2}$, & $\sqrt{3} \mapsto \sqrt{3}$; \\
		$\sigma_2$ : & $\sqrt{2} \mapsto -\sqrt{2}$, & $\sqrt{3} \mapsto -\sqrt{3}$; \\
		$\sigma_3$ : & $\sqrt{2} \mapsto \sqrt{2}$, & $\sqrt{3} \mapsto \sqrt{3}$; \\
		$\sigma_4$ : & $\sqrt{2} \mapsto -\sqrt{2}$, & $\sqrt{3} \mapsto -\sqrt{3}$.
		\end{tabular}
	\end{center}
	Since the order of $G$ is 4, each of rhese possiblilites gives rise to an actual automorphism of $E$. We have
	\[
		\sigma_2^2 = \sigma_3^2 = \sigma_4^2 = \text{id},
	\]
	and
	\begin{align*}
		\sigma_3 \sigma_2 &= \sigma_2 \sigma_3 = \sigma_4, \\
		\sigma_2 \sigma_4 &= \sigma_4 \sigma_2 = \sigma_3, \\
		\sigma_3 \sigma_4 &= \sigma_4 \sigma_3 = \sigma_2.
	\end{align*}
	So $G = \langle \sigma_2 \rangle \times \langle \sigma_3 \rangle$ is the \defn{Klein four group}. The Galois correspondence is as follows:
	\[
		\begin{matrix}
			\mathcal{H} & : & G & \langle \sigma_2 \rangle & \langle \sigma_3 \rangle & \langle \sigma_2 \sigma_3 \rangle & \{1\} \\
			\mathcal{B} & : & \Q & \Q(\sqrt{3}) & \Q(\sqrt{2}) & \Q(\sqrt{6}) & E.
		\end{matrix}
	\]
	As an example, $\langle \sigma_2 \rangle$ fixes $\sqrt{3}$ and has order 2 so, by the Fundamental Theorem of Galois Theory, the corresponding intermediate field which also fixes $\sqrt{3}$ and has degree 2 is $\Q(\sqrt{3})$.
	
	\emph{An alternative method} -- without making use of the Fundamental Theorem of Galois Theory: We can write $E = \{a + b\sqrt{2} + c\sqrt{3} + d\sqrt{6} \mid a, b, c, d \in \Q\}$. If we take, say, $\sigma_4$ and apply it to an arbitray element of $E$, we get
	\[
		\sigma_4(a + b\sqrt{2} + c\sqrt{3} + d\sqrt{6}) = a - b\sqrt{2} - c\sqrt{3} + d\sqrt{6}.
	\]
	Comparing coefficients, we see that fixed elements under $\sigma_4$ are of the form $a + d\sqrt{6}$, $a, d \in \Q$.
\end{example}

\begin{example}
	Let $E$ be the splitting field of $f = x^3 - 2$ over $\Q$. Thus, $E = \Q(\xi,\, \varepsilon \xi,\, \overline{\varepsilon} \xi)$, where $\xi = \sqrt[3]{2}$ is the real cube root of 2, and $\varepsilon$ and $\overline{\varepsilon}$ are the non-real complex roots of 1 ($\overline{\varepsilon} = \varepsilon$).
	
	Then $\varepsilon \xi / \overline{\varepsilon} \xi = \varepsilon \in E$, and hence $E = \Q(\xi, \varepsilon)$. Now, $f$ is irreducible over $\Q$, so $(\Q(\xi), \Q) = 3$. Moreover, $\varepsilon, \overline{\varepsilon}$ are roots of $x^2 + x + 1$ which is irreducible over $\Q$, but it stays irreducible over $\Q(\xi)$ (because this field consists entirely of real numbers). Therefore, $(E : \Q(\xi)) = 2$ and by the Tower Law, $(E : \Q) = (E : \Q(\xi))(\Q(\xi) : \Q) = 6$. So $E$ is a splitting field of $f$, and so the Fundamental Theorem of Galois Theory applies to $E : \Q$. Therefore the Galois group $G = \Gamma(E : \Q)$ is a group of order 6.
	
	Once again, the elements of $G$ are completely determined by their effect on $\xi$ and $\varepsilon$. Since $\xi$ must be mapped to a root of $x^3 - 2$, and $\varepsilon$ must be mapped to a root of $x^2 + x + 1$, the possibilities are:
	\[
		\xi \mapsto \xi,\ \varepsilon \xi,\ \overline{\varepsilon} \xi, \qquad \varepsilon \mapsto \varepsilon,\ \overline{\varepsilon}.
	\]
	Therefore each of these 6 possibilities give rise to an actual automorphism.
\end{example}
